{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🦠 Emergent Molecular Communication\n",
    "\n",
    "A preliminary study of emergent molecular communication protocols learned by graph-based agents in a diffusion channel environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Imports](#introduction)\n",
    "- [Utils](#utils)\n",
    "  - [Data](#data)\n",
    "  - [Neural Networks](#neural-networks)\n",
    "  - [Communication Model](#communication-model)\n",
    "  - [Train the Communication Model](#train-the-communication-model)\n",
    "- [Running the Experiments](#running-the-experiments)\n",
    "  - [Experiments on Ideal and Diffusion Channel](#experiments-on-ideal-and-diffusion-channel)\n",
    "  - [Experiments on Pure Relay Nodes](#experiments-on-pure-relay-nodes)\n",
    "- [Collect and Visualize Results](#collect-and-visualize-results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "import logging\n",
    "from logging import getLogger\n",
    "import math\n",
    "import random\n",
    "from random import shuffle\n",
    "from itertools import product\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from functools import cache\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from scipy.special import erfc\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import ray\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(\n",
    "    n_nodes: int, density: float, random_state: Optional[int] = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate random (x, y) positions for nodes in a graph.\n",
    "\n",
    "    The nodes are uniformly distributed in a square region.\n",
    "    The side length of the square is computed based on the given density,\n",
    "    so that the area of the square is n_nodes/density.\n",
    "\n",
    "    Parameters:\n",
    "        n_nodes (int): Number of nodes.\n",
    "        density (float): Average number of nodes per unit square.\n",
    "        random_state (int): Seed for the random number generator.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of shape (n_nodes, 2) with the (x, y) positions.\n",
    "    \"\"\"\n",
    "    # Initialize the random number generator with the provided seed.\n",
    "    if random_state is not None:\n",
    "        rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # Compute the side length of the square domain.\n",
    "    side_length = np.sqrt(n_nodes / density)\n",
    "\n",
    "    # Generate positions uniformly in the square [0, side_length] x [0, side_length].\n",
    "    positions = rng.uniform(low=0, high=side_length, size=(n_nodes, 2))\n",
    "\n",
    "    return positions\n",
    "\n",
    "\n",
    "def get_sender_receiver(graph: np.ndarray) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Return the index of the sender and receiver nodes. The sender and the receiver are the nodes\n",
    "    that are furthest apart in the graph.\n",
    "\n",
    "    Parameters:\n",
    "        graph (np.ndarray): An array of shape (n_nodes, 2) with the (x, y) positions of the nodes.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[int, int]: The indices of the sender and receiver nodes.\n",
    "    \"\"\"\n",
    "    # Compute the pairwise distances between all nodes.\n",
    "    distances = np.linalg.norm(graph[:, None] - graph, axis=-1)\n",
    "\n",
    "    # Find the indices of the sender and receiver nodes.\n",
    "    sender, receiver = np.unravel_index(np.argmax(distances), distances.shape)\n",
    "\n",
    "    return sender.item(), receiver.item()\n",
    "\n",
    "\n",
    "def get_distance_matrix(graph: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the pairwise distance matrix between all nodes in the graph.\n",
    "\n",
    "    Parameters:\n",
    "        graph (np.ndarray): An array of shape (n_nodes, 2) with the (x, y) positions of the nodes.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A distance matrix of shape (n_nodes, n_nodes) where the entry (i, j) is the\n",
    "            Euclidean distance between nodes i and j.\n",
    "    \"\"\"\n",
    "    # Compute the pairwise distances between all nodes.\n",
    "    distances = np.linalg.norm(graph[:, None] - graph, axis=-1)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "def plot_graph(graph: np.ndarray, ax=None):\n",
    "    \"\"\"\n",
    "    Plot the graph with nodes and edges.\n",
    "\n",
    "    Parameters:\n",
    "        graph (np.ndarray): An array of shape (n_nodes, 2) with the (x, y) positions of the nodes.\n",
    "        ax (plt.Axes): The axes where to plot the graph.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "\n",
    "    # Plot the nodes.\n",
    "    ax.plot(graph[:, 0], graph[:, 1], \"o\", label=\"Nodes\")\n",
    "    for i, (x, y) in enumerate(graph):\n",
    "        ax.text(x, y, f\"{i}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    # Plot the sender and the receiver nodes with a different color.\n",
    "    sender, receiver = get_sender_receiver(graph)\n",
    "    ax.plot(graph[sender, 0], graph[sender, 1], \"o\", color=\"green\", label=\"Sender\")\n",
    "    ax.plot(graph[receiver, 0], graph[receiver, 1], \"o\", color=\"red\", label=\"Receiver\")\n",
    "\n",
    "    ax.set_aspect(\"equal\", \"box\")\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_title(\"Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    \"iris\",\n",
    "    \"wine\",\n",
    "    \"breast_cancer\",\n",
    "]\n",
    "\n",
    "def get_classification_data(\n",
    "    dataset_name: str,\n",
    "    test_size: float = 0.2,\n",
    "    val_size: float = 0.1,\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a dataset from OpenML (UCI repository) and preprocess it:\n",
    "    - Detect numerical and categorical features\n",
    "    - Impute missing values\n",
    "    - One-hot encode categorical features\n",
    "    - Standardize all features\n",
    "    - Encode target labels\n",
    "    - Split into train/validation/test sets\n",
    "\n",
    "    Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test as numpy arrays\n",
    "    \"\"\"\n",
    "    # Check if the dataset is in the list of available datasets\n",
    "    if dataset_name not in DATASETS:\n",
    "        raise ValueError(\n",
    "            f\"Dataset {dataset_name} is not available. Please choose from {DATASETS}.\"\n",
    "        )\n",
    "\n",
    "    # Check if the dataset file exists\n",
    "    dataset_path = os.path.join(\"..\", \"data\", f\"{dataset_name}.npz\")\n",
    "    if os.path.exists(dataset_path):\n",
    "        with np.load(dataset_path, allow_pickle=False) as data:\n",
    "            X_train = data[\"X_train\"]\n",
    "            X_val = data[\"X_val\"]\n",
    "            X_test = data[\"X_test\"]\n",
    "            y_train = data[\"y_train\"]\n",
    "            y_val = data[\"y_val\"]\n",
    "            y_test = data[\"y_test\"]\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "    # fetch dataset\n",
    "    ids = {\n",
    "        \"iris\": 53,\n",
    "        \"wine\": 109,\n",
    "        \"breast_cancer\": 17,\n",
    "    }\n",
    "    if dataset_name not in DATASETS:\n",
    "        raise ValueError(\n",
    "            f\"Dataset {dataset_name} is not available. Please choose from {DATASETS}.\"\n",
    "        )\n",
    "\n",
    "    ds = fetch_ucirepo(id=ids[dataset_name])\n",
    "    X = ds.data.features\n",
    "    y = ds.data.targets\n",
    "\n",
    "    # Identify feature types\n",
    "    numeric_features = X.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    categorical_features = [col for col in X.columns if col not in numeric_features]\n",
    "\n",
    "    # Pipelines for numerical and categorical features\n",
    "    numeric_pipeline = Pipeline(\n",
    "        [(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    "    )\n",
    "    categorical_pipeline = Pipeline(\n",
    "        [\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        [\n",
    "            (\"num\", numeric_pipeline, numeric_features),\n",
    "            (\"cat\", categorical_pipeline, categorical_features),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Encode target labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=test_size, random_state=random_state, stratify=y_encoded\n",
    "    )\n",
    "    # Adjust validation size relative to the remaining data\n",
    "    val_relative_size = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp,\n",
    "        y_temp,\n",
    "        test_size=val_relative_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y_temp,\n",
    "    )\n",
    "\n",
    "    # Fit the preprocessor on the training data and transform all sets\n",
    "    if dataset_name in ids:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            X_train_processed = preprocessor.fit_transform(X_train)\n",
    "            X_val_processed = preprocessor.transform(X_val)\n",
    "            X_test_processed = preprocessor.transform(X_test)\n",
    "    else:\n",
    "        X_train_processed = X_train\n",
    "        X_val_processed = X_val\n",
    "        X_test_processed = X_test\n",
    "\n",
    "    # Save the dataset as a numpy array\n",
    "    os.makedirs(os.path.join(\"..\", \"data\"), exist_ok=True)\n",
    "    np.savez_compressed(\n",
    "        os.path.join(\"..\", \"data\", f\"{dataset_name}.npz\"),\n",
    "        X_train=X_train_processed,\n",
    "        X_val=X_val_processed,\n",
    "        X_test=X_test_processed,\n",
    "        y_train=y_train,\n",
    "        y_val=y_val,\n",
    "        y_test=y_test,\n",
    "        allow_pickle=False,\n",
    "    )\n",
    "\n",
    "    return X_train_processed, X_val_processed, X_test_processed, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "data_stats_dict = {\n",
    "    \"name\": [],\n",
    "    \"n_samples\": [],\n",
    "    \"n_features\": [],\n",
    "    \"n_classes\": [],\n",
    "}\n",
    "for dataset in DATASETS:\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = get_classification_data(dataset)\n",
    "    data_stats_dict[\"name\"].append(dataset)\n",
    "    data_stats_dict[\"n_samples\"].append(\n",
    "        X_train.shape[0] + X_val.shape[0] + X_test.shape[0]\n",
    "    )\n",
    "    data_stats_dict[\"n_features\"].append(X_train.shape[1])\n",
    "    data_stats_dict[\"n_classes\"].append(len(np.unique(y_train)))\n",
    "data_stats_df = pd.DataFrame(data_stats_dict)\n",
    "data_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        hidden_dims: list = [4],\n",
    "        dropout_rate: float = 0.0,\n",
    "        use_batchnorm: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Feedforward Neural Network with ReLU activations and Dropout.\n",
    "\n",
    "        Parameters:\n",
    "            in_features (int): Dimensionality of the input features.\n",
    "            out_features (int): Dimensionality of the output features.\n",
    "            hidden_dims (list): List with the number of neurons for each hidden layer.\n",
    "            dropout_rate (float): Dropout probability.\n",
    "            use_batchnorm (bool): Whether to use Batch Normalization.\n",
    "            residual_coefficient (float): Coefficient for the residual connection.\n",
    "            output_coefficient (float): Coefficient for the output layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = in_features\n",
    "        # Build hidden layers\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_dim = h\n",
    "        layers.append(nn.Linear(prev_dim, out_features))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Parameters:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_encoder_decoder(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    n_features,\n",
    "    n_classes,\n",
    "    hidden_dims,\n",
    "    use_batchnorm,\n",
    "    lr=1e-3,\n",
    "    n_epochs=50,\n",
    "    patience=5,\n",
    "    lr_drop_after=0.5,\n",
    "    warmup_epochs=10,\n",
    "    l1_lambda=0.01,\n",
    "    batch_size=256,\n",
    "    device=\"cpu\",\n",
    "    weights_dir=\"enc_dec_weights\",\n",
    "    verbose=False,\n",
    "):\n",
    "    os.makedirs(weights_dir, exist_ok=True)\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Build the two networks\n",
    "    n_chem = n_classes\n",
    "    encoder = FeedForwardNeuralNetwork(\n",
    "        in_features=n_features,\n",
    "        out_features=n_chem,\n",
    "        hidden_dims=hidden_dims,\n",
    "        use_batchnorm=use_batchnorm,\n",
    "    ).to(device)\n",
    "    decoder = FeedForwardNeuralNetwork(\n",
    "        in_features=n_chem,\n",
    "        out_features=n_classes,\n",
    "        hidden_dims=hidden_dims,\n",
    "        use_batchnorm=use_batchnorm,\n",
    "    ).to(device)\n",
    "\n",
    "    # Joint optimizer & loss\n",
    "    optimizer = optim.Adam(\n",
    "        list(encoder.parameters()) + list(decoder.parameters()), lr=lr\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # DataLoaders\n",
    "    train_ds = TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.long),\n",
    "    )\n",
    "    val_ds = TensorDataset(\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.long),\n",
    "    )\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    best_enc = deepcopy(encoder.state_dict())\n",
    "    best_dec = deepcopy(decoder.state_dict())\n",
    "    no_imp = 0\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # — train epoch —\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        running = total = 0.0\n",
    "        for Xb, yb in train_loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            chem = encoder(Xb)\n",
    "            logits = decoder(chem)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "            # Add L1 regularization\n",
    "            l1_norm = sum(p.abs().sum() for p in encoder.parameters()) + sum(\n",
    "                p.abs().sum() for p in decoder.parameters()\n",
    "            )\n",
    "            loss += l1_lambda * l1_norm\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running += loss.item() * Xb.size(0)\n",
    "            total += Xb.size(0)\n",
    "\n",
    "        # — validation epoch —\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        vrun = vtot = 0.0\n",
    "        with torch.no_grad():\n",
    "            for Xb, yb in val_loader:\n",
    "                Xb, yb = Xb.to(device), yb.to(device)\n",
    "                vchem = encoder(Xb)\n",
    "                vlogits = decoder(vchem)\n",
    "                vrun += criterion(vlogits, yb).item() * Xb.size(0)\n",
    "                vtot += Xb.size(0)\n",
    "\n",
    "        train_loss = running / total\n",
    "        val_loss = vrun / vtot\n",
    "        val_acc = (vlogits.argmax(dim=1) == yb).float().mean().item()\n",
    "        if verbose:\n",
    "            logger.info(\n",
    "                f\"[EncDec] Epoch {epoch}: train={train_loss:.4f} val={val_loss:.4f} acc={val_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "        # early stop\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_enc = deepcopy(encoder.state_dict())\n",
    "            best_dec = deepcopy(decoder.state_dict())\n",
    "            no_imp = 0\n",
    "        else:\n",
    "            no_imp += 1\n",
    "            if no_imp >= patience and epoch > warmup_epochs:\n",
    "                if verbose:\n",
    "                    logger.warning(\"Encoder-decoder pretraining stopped early.\")\n",
    "                break\n",
    "\n",
    "        # learning rate drop\n",
    "        drop_epoch = math.ceil(n_epochs * lr_drop_after)\n",
    "        if epoch > warmup_epochs and epoch == drop_epoch:\n",
    "            for g in optimizer.param_groups:\n",
    "                g[\"lr\"] = lr / 10\n",
    "            if verbose:\n",
    "                logger.info(f\"Learning rate dropped to {lr / 10:.1e} at epoch {epoch}.\")\n",
    "\n",
    "    # save best\n",
    "    encoder.load_state_dict(best_enc)\n",
    "    decoder.load_state_dict(best_dec)\n",
    "    enc_path = os.path.join(weights_dir, f\"{dataset_name}_encoder_weights.pth\")\n",
    "    dec_path = os.path.join(weights_dir, f\"{dataset_name}_decoder_weights.pth\")\n",
    "    torch.save(encoder.state_dict(), enc_path)\n",
    "    torch.save(decoder.state_dict(), dec_path)\n",
    "    if verbose:\n",
    "        logger.debug(f\"Saved encoder → {enc_path}\")\n",
    "        logger.debug(f\"Saved decoder → {dec_path}\")\n",
    "\n",
    "    # test\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    correct, total = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in test_loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            tchem = encoder(Xb)\n",
    "            tlogits = decoder(tchem)\n",
    "            correct += (tlogits.argmax(dim=1) == yb).float().sum().item()\n",
    "            total += Xb.size(0)\n",
    "    test_acc = correct / total\n",
    "    return enc_path, dec_path, test_acc\n",
    "\n",
    "\n",
    "os.makedirs(\"enc_dec_weights\", exist_ok=True)\n",
    "res_csv_path = os.path.join(\"enc_dec_weights\", \"enc_dec_results.csv\")\n",
    "if os.path.exists(res_csv_path):\n",
    "    df = pd.read_csv(res_csv_path)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=[\"dataset\", \"n_features\", \"n_classes\", \"acc\"])\n",
    "    df.to_csv(res_csv_path, index=False)\n",
    "\n",
    "for dataset_name in DATASETS:\n",
    "    if dataset_name in [\"identity\", \"shift\", \"negate\"]:\n",
    "        continue\n",
    "    enc_path = os.path.join(\"enc_dec_weights\", f\"{dataset_name}_encoder_weights.pth\")\n",
    "    dec_path = os.path.join(\"enc_dec_weights\", f\"{dataset_name}_decoder_weights.pth\")\n",
    "    if not os.path.exists(enc_path) or not os.path.exists(dec_path):\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = get_classification_data(\n",
    "            dataset_name\n",
    "        )\n",
    "        n_features = X_train.shape[1]\n",
    "        n_classes = len(np.unique(y_train))\n",
    "        _, _, acc = train_encoder_decoder(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_val,\n",
    "            y_val,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            n_features,\n",
    "            n_classes,\n",
    "            hidden_dims=[],\n",
    "            use_batchnorm=False,\n",
    "            lr=1e-3,\n",
    "            n_epochs=10000,\n",
    "            patience=100,\n",
    "            batch_size=1024,\n",
    "            device=\"cpu\",\n",
    "            weights_dir=\"enc_dec_weights\",\n",
    "        )\n",
    "        logger.info(f\"Trained encoder-decoder on {dataset_name}. Acc: {acc:.4f}\")\n",
    "        df = pd.read_csv(res_csv_path)\n",
    "        new_row = pd.DataFrame(\n",
    "            {\n",
    "                \"dataset\": [dataset_name],\n",
    "                \"n_features\": [n_features],\n",
    "                \"n_classes\": [n_classes],\n",
    "                \"acc\": [acc],\n",
    "            }\n",
    "        )\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.to_csv(res_csv_path, index=False)\n",
    "    else:\n",
    "        logger.info(f\"Encoder-decoder weights already exist for {dataset_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communication Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_passing(\n",
    "    message: torch.Tensor,\n",
    "    distance: torch.Tensor,\n",
    "    phi: nn.ModuleList,\n",
    "    residual_coefficient: float = 0.0,\n",
    "    output_coefficient: float = 1.0,\n",
    "    use_diffusion_noise: bool = True,\n",
    "    memory: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Perform one message passing iteration, optionally incorporating memory.\n",
    "\n",
    "    Parameters:\n",
    "        message: Tensor of shape (B, n, d)  -- messages at current iteration\n",
    "        distance: Tensor of shape (n, n)     -- distance matrix between nodes\n",
    "        phi:     ModuleList of n mappings   -- each maps R^{d or 2d} -> R^d\n",
    "        residual_coefficient: float         -- coefficient for residual connection\n",
    "        output_coefficient: float          -- coefficient for output layer\n",
    "        use_diffusion_noise: bool          -- whether to add diffusion noise\n",
    "        memory:  bool                        -- whether to concatenate previous message\n",
    "\n",
    "    Returns:\n",
    "        new_message: Tensor of shape (B, n, d) -- updated messages\n",
    "    \"\"\"\n",
    "    B, N, D = message.shape\n",
    "\n",
    "    # Save previous messages if memory is on\n",
    "    prev_msg = message.clone() if memory else None\n",
    "\n",
    "    # Compute detection probabilities\n",
    "    p = torch.erfc(distance / 2.0)  # (n, n)\n",
    "    p_exp = p.unsqueeze(0).unsqueeze(-1).expand(B, N, N, D)  # (B,n,n,d)\n",
    "\n",
    "    # Channel noise\n",
    "    if use_diffusion_noise:\n",
    "        eps = torch.randn(B, N, N, D, device=message.device)\n",
    "        Delta = (p_exp * (1 + eps - eps * p_exp)).clip(min=0)  # (B,n,n,d)\n",
    "    else:\n",
    "        Delta = torch.ones_like(p_exp, device=message.device)\n",
    "\n",
    "    # Compute aggregated messages\n",
    "    aggregated = torch.einsum(\"bijk,bjk->bik\", Delta, message)  # (B,n,d)\n",
    "\n",
    "    # Compute new messages per node\n",
    "    new_message = torch.zeros_like(message, device=message.device)\n",
    "    for i in range(N):\n",
    "        if memory:\n",
    "            # concat along feature dim: [aggregated, previous]\n",
    "            inp = torch.cat([aggregated[:, i, :], prev_msg[:, i, :]], dim=-1)\n",
    "        else:\n",
    "            inp = aggregated[:, i, :]\n",
    "        new_message[:, i, :] = phi[i](inp)\n",
    "\n",
    "    # Apply residual connection\n",
    "    new_message = residual_coefficient * message + output_coefficient * new_message\n",
    "    return new_message\n",
    "\n",
    "\n",
    "class MolComNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph: np.ndarray,\n",
    "        n_features: int,\n",
    "        n_classes: int,\n",
    "        encoder: torch.nn.Module = None,\n",
    "        n_iters: int = 1,\n",
    "        hidden_dims: list = [4],\n",
    "        dropout_rate: float = 0.0,\n",
    "        use_batchnorm: bool = False,\n",
    "        residual_coefficient: float = 0.0,\n",
    "        output_coefficient: float = 1.0,\n",
    "        use_diffusion_noise: bool = True,\n",
    "        memory: bool = False,\n",
    "        freeze_encoders: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.graph = graph\n",
    "        self.graph_size = len(graph)\n",
    "        self.distances = torch.tensor(get_distance_matrix(graph), dtype=torch.float32)\n",
    "        self.sender, self.receiver = get_sender_receiver(graph)\n",
    "        self.n_iters = n_iters\n",
    "        self.residual_coefficient = residual_coefficient\n",
    "        self.output_coefficient = output_coefficient\n",
    "        self.n_classes = n_classes\n",
    "        self.use_diffusion_noise = use_diffusion_noise\n",
    "        self.memory = memory\n",
    "\n",
    "        # Encoder unchanged\n",
    "        self.encoder = encoder\n",
    "        if freeze_encoders and encoder is not None:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # φ‐networks: double in_features if memory=True\n",
    "        phi_in = 2 * n_classes if memory else n_classes\n",
    "        self.phi = nn.ModuleList(\n",
    "            [\n",
    "                FeedForwardNeuralNetwork(\n",
    "                    in_features=phi_in,\n",
    "                    out_features=n_classes,\n",
    "                    hidden_dims=hidden_dims,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    use_batchnorm=use_batchnorm,\n",
    "                )\n",
    "                for _ in range(self.graph_size)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B = x.size(0)\n",
    "        self.distances = self.distances.to(x.device)\n",
    "        assert (\n",
    "            x.shape[1] == self.n_features\n",
    "        ), f\"Expected {self.n_features} features, got {x.shape[1]}\"\n",
    "\n",
    "        # Initialize messages: only sender nodes get the encoded signal\n",
    "        msg = torch.zeros(B, self.graph_size, self.n_classes, device=x.device)\n",
    "        msg[:, self.sender, :] = self.encoder(x) if self.encoder is not None else x\n",
    "        msg = torch.relu(msg)\n",
    "        out = torch.zeros(B, self.n_classes, self.n_iters, device=x.device)\n",
    "\n",
    "        # Iterative message passing\n",
    "        for i in range(self.n_iters):\n",
    "            msg = message_passing(\n",
    "                msg,\n",
    "                self.distances,\n",
    "                self.phi,\n",
    "                residual_coefficient=self.residual_coefficient,\n",
    "                output_coefficient=self.output_coefficient,\n",
    "                memory=self.memory,\n",
    "                use_diffusion_noise=self.use_diffusion_noise,\n",
    "            )\n",
    "            msg = torch.relu(msg)\n",
    "            # Save the output of the last iteration\n",
    "            out[:, :, i] = msg[:, self.receiver, :]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Communication Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iter_metrics(\n",
    "    outputs: torch.Tensor, targets: torch.Tensor, k: int\n",
    ") -> dict[str, float | list[float]]:\n",
    "    \"\"\"\n",
    "    Returns a dict with:\n",
    "      - acc_last       float\n",
    "      - acc_avg        float\n",
    "      - mean_first     float\n",
    "      - conv_rate      float\n",
    "      - stability      float\n",
    "      - acc_per_iter   list[float]\n",
    "    \"\"\"\n",
    "    B, C, T = outputs.shape\n",
    "\n",
    "    preds = outputs.argmax(dim=1)  # (B, T)\n",
    "    correct = preds.eq(targets.unsqueeze(1))  # (B, T)\n",
    "    acc_per_iter = correct.float().mean(dim=0).tolist()\n",
    "\n",
    "    acc_last = acc_per_iter[-1]\n",
    "\n",
    "    probs = F.softmax(outputs, dim=1)  # (B, C, T)\n",
    "    acc_avg = (probs.mean(dim=2).argmax(dim=1) == targets).float().mean().item()\n",
    "\n",
    "    idx = torch.arange(T, device=outputs.device).unsqueeze(0).expand(B, T)\n",
    "    first_idx = torch.where(correct, idx, torch.full_like(idx, T)).min(dim=1)[0] + 1\n",
    "    valid = first_idx <= T\n",
    "    mean_first = first_idx[valid].float().mean().item() if valid.any() else float(\"nan\")\n",
    "\n",
    "    k_clamped = max(1, min(k, T))\n",
    "    stable = (preds[:, k_clamped - 1].unsqueeze(1) == preds[:, k_clamped - 1 :]).all(\n",
    "        dim=1\n",
    "    )\n",
    "    conv_rate = stable.float().mean().item()\n",
    "\n",
    "    flips = (preds[:, 1:] != preds[:, :-1]).float().sum(dim=1)\n",
    "    stability = 1 - (flips / (T - 1)).mean().item()\n",
    "\n",
    "    return {\n",
    "        \"acc_last\": acc_last,\n",
    "        \"acc_avg\": acc_avg,\n",
    "        \"mean_first\": mean_first,\n",
    "        \"conv_rate\": conv_rate,\n",
    "        \"stability\": stability,\n",
    "        \"acc_per_iter\": acc_per_iter,\n",
    "    }\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, k):\n",
    "    model.train()\n",
    "    meters = {\n",
    "        \"loss\": 0.0,\n",
    "        \"acc_last\": 0.0,\n",
    "        \"acc_avg\": 0.0,\n",
    "        \"mean_first\": 0.0,\n",
    "        \"conv_rate\": 0.0,\n",
    "        \"stability\": 0.0,\n",
    "        \"acc_per_iter\": None,  # will become list\n",
    "        \"count\": 0,\n",
    "    }\n",
    "\n",
    "    for X, y in loader:\n",
    "        B = X.size(0)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(X)  # (B, C, T)\n",
    "        T = out.size(2)\n",
    "\n",
    "        loss = sum(criterion(out[:, :, t], y) for t in range(T)) / T\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        metrics = compute_iter_metrics(out, y, k)\n",
    "\n",
    "        meters[\"loss\"] += loss.item() * B\n",
    "        meters[\"acc_last\"] += metrics[\"acc_last\"] * B\n",
    "        meters[\"acc_avg\"] += metrics[\"acc_avg\"] * B\n",
    "        meters[\"mean_first\"] += metrics[\"mean_first\"] * B\n",
    "        meters[\"conv_rate\"] += metrics[\"conv_rate\"] * B\n",
    "        meters[\"stability\"] += metrics[\"stability\"] * B\n",
    "\n",
    "        if meters[\"acc_per_iter\"] is None:\n",
    "            meters[\"acc_per_iter\"] = torch.zeros(T, dtype=torch.float64)\n",
    "        meters[\"acc_per_iter\"] += (\n",
    "            torch.tensor(metrics[\"acc_per_iter\"], dtype=torch.float64) * B\n",
    "        )\n",
    "\n",
    "        meters[\"count\"] += B\n",
    "\n",
    "    # normalize\n",
    "    N = meters[\"count\"]\n",
    "    epoch = {\n",
    "        k: (v / N if k != \"acc_per_iter\" else (meters[\"acc_per_iter\"] / N).tolist())\n",
    "        for k, v in meters.items()\n",
    "        if k != \"count\"\n",
    "    }\n",
    "    return epoch\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(model, loader, criterion, device, k):\n",
    "    model.eval()\n",
    "    meters = {\n",
    "        \"loss\": 0.0,\n",
    "        \"acc_last\": 0.0,\n",
    "        \"acc_avg\": 0.0,\n",
    "        \"mean_first\": 0.0,\n",
    "        \"conv_rate\": 0.0,\n",
    "        \"stability\": 0.0,\n",
    "        \"acc_per_iter\": None,\n",
    "        \"count\": 0,\n",
    "    }\n",
    "\n",
    "    for X, y in loader:\n",
    "        B = X.size(0)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        out = model(X)\n",
    "        T = out.size(2)\n",
    "        loss = sum(criterion(out[:, :, t], y) for t in range(T)) / T\n",
    "\n",
    "        metrics = compute_iter_metrics(out, y, k)\n",
    "\n",
    "        meters[\"loss\"] += loss.item() * B\n",
    "        meters[\"acc_last\"] += metrics[\"acc_last\"] * B\n",
    "        meters[\"acc_avg\"] += metrics[\"acc_avg\"] * B\n",
    "        meters[\"mean_first\"] += metrics[\"mean_first\"] * B\n",
    "        meters[\"conv_rate\"] += metrics[\"conv_rate\"] * B\n",
    "        meters[\"stability\"] += metrics[\"stability\"] * B\n",
    "\n",
    "        if meters[\"acc_per_iter\"] is None:\n",
    "            meters[\"acc_per_iter\"] = torch.zeros(T, dtype=torch.float64)\n",
    "        meters[\"acc_per_iter\"] += (\n",
    "            torch.tensor(metrics[\"acc_per_iter\"], dtype=torch.float64) * B\n",
    "        )\n",
    "\n",
    "        meters[\"count\"] += B\n",
    "\n",
    "    N = meters[\"count\"]\n",
    "    epoch = {\n",
    "        k: (v / N if k != \"acc_per_iter\" else (meters[\"acc_per_iter\"] / N).tolist())\n",
    "        for k, v in meters.items()\n",
    "        if k != \"count\"\n",
    "    }\n",
    "    return epoch\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    n_epochs: int = 100,\n",
    "    patience: int = 10,\n",
    "    warmup_epochs: int = 25,\n",
    "    lr_drop_after: float = 0.5,\n",
    "    lr: float = 1e-3,\n",
    "    device: str = \"cpu\",\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    warmup_epochs = min(warmup_epochs, n_epochs)\n",
    "    lr_drop_epoch = math.ceil(n_epochs * lr_drop_after)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        device = torch.device(device)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_state = deepcopy(model.state_dict())\n",
    "        no_improve = 0\n",
    "\n",
    "        # Initialize history\n",
    "        history: dict[str, dict[str, list]] = {\n",
    "            \"train\": {\n",
    "                k: []\n",
    "                for k in [\n",
    "                    \"loss\",\n",
    "                    \"acc_last\",\n",
    "                    \"acc_avg\",\n",
    "                    \"mean_first\",\n",
    "                    \"conv_rate\",\n",
    "                    \"stability\",\n",
    "                    \"acc_per_iter\",\n",
    "                ]\n",
    "            },\n",
    "            \"val\": {\n",
    "                k: []\n",
    "                for k in [\n",
    "                    \"loss\",\n",
    "                    \"acc_last\",\n",
    "                    \"acc_avg\",\n",
    "                    \"mean_first\",\n",
    "                    \"conv_rate\",\n",
    "                    \"stability\",\n",
    "                    \"acc_per_iter\",\n",
    "                ]\n",
    "            },\n",
    "        }\n",
    "\n",
    "        k = int(math.sqrt(model.graph_size))\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            train_metrics = train_one_epoch(\n",
    "                model, train_loader, optimizer, criterion, device, k\n",
    "            )\n",
    "            val_metrics = eval_one_epoch(model, val_loader, criterion, device, k)\n",
    "\n",
    "            # Log and store\n",
    "            for phase, metrics in [(\"train\", train_metrics), (\"val\", val_metrics)]:\n",
    "                for name, val in metrics.items():\n",
    "                    history[phase][name].append(val)\n",
    "                # Logging with loguru\n",
    "            if verbose:\n",
    "                logger.info(\n",
    "                    f\"Epoch {epoch:03} | \"\n",
    "                    f\"Train L={train_metrics['loss']:.4f}, last@={train_metrics['acc_last']:.3f}, \"\n",
    "                    f\"avg@={train_metrics['acc_avg']:.3f}, conv@k={train_metrics['conv_rate']:.3f}, \"\n",
    "                    f\"stab={train_metrics['stability']:.3f}, \"\n",
    "                    f\"mean_first={train_metrics['mean_first']:.3f} | \"\n",
    "                    f\"Val   L={val_metrics['loss']:.4f}, last@={val_metrics['acc_last']:.3f}, \"\n",
    "                    f\"avg@={val_metrics['acc_avg']:.3f}, conv@k={val_metrics['conv_rate']:.3f}, \"\n",
    "                    f\"stab={val_metrics['stability']:.3f}, \"\n",
    "                    f\"mean_first={val_metrics['mean_first']:.3f}\"\n",
    "                )\n",
    "\n",
    "            # Early stopping\n",
    "            if val_metrics[\"loss\"] < best_val_loss:\n",
    "                best_val_loss = val_metrics[\"loss\"]\n",
    "                best_state = deepcopy(model.state_dict())\n",
    "                no_improve = 0\n",
    "                if verbose:\n",
    "                    logger.debug(\n",
    "                        f\"Epoch {epoch}: New best val loss: {best_val_loss:.4f}\"\n",
    "                    )\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= patience and epoch > warmup_epochs:\n",
    "                    if verbose:\n",
    "                        logger.info(\n",
    "                            f\"Early stopping at epoch {epoch} with patience {patience}.\"\n",
    "                        )\n",
    "                    break\n",
    "\n",
    "            # Learning rate drop\n",
    "            if epoch > warmup_epochs and epoch == lr_drop_epoch:\n",
    "                for g in optimizer.param_groups:\n",
    "                    g[\"lr\"] = lr / 10\n",
    "                if verbose:\n",
    "                    logger.info(\n",
    "                        f\"Learning rate dropped to {lr / 10:.1e} at epoch {epoch}.\"\n",
    "                    )\n",
    "\n",
    "        # Restore best and test\n",
    "        model.load_state_dict(best_state)\n",
    "        test_metrics = eval_one_epoch(model, test_loader, criterion, device, k)\n",
    "        if verbose:\n",
    "            logger.info(\n",
    "                f\"Test L={test_metrics['loss']:.4f}, last@={test_metrics['acc_last']:.3f}, \"\n",
    "                f\"avg@={test_metrics['acc_avg']:.3f}, conv@k={test_metrics['conv_rate']:.3f}, \"\n",
    "                f\"stab={test_metrics['stability']:.3f}, \"\n",
    "                f\"mean_first={test_metrics['mean_first']:.3f}\"\n",
    "            )\n",
    "\n",
    "    return history, test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(\n",
    "    X: np.ndarray, y: np.ndarray, batch_size: int = 1, shuffle: bool = True\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Create a DataLoader from the features and targets.\n",
    "\n",
    "    Parameters:\n",
    "        X (np.ndarray): Features.\n",
    "        y (np.ndarray): Targets.\n",
    "        batch_size (int): Batch size.\n",
    "        shuffle (bool): Whether to shuffle the data.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: PyTorch DataLoader.\n",
    "    \"\"\"\n",
    "    dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X, dtype=torch.float32), torch.tensor(y)\n",
    "    )\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_run(\n",
    "    n_nodes: int = 10,\n",
    "    density: float = 1,\n",
    "    dataset_name: str = \"iris\",\n",
    "    n_iters: int = 10,\n",
    "    hidden_dims: list = [4],\n",
    "    dropout_rate: float = 0.0,\n",
    "    use_batchnorm: bool = False,\n",
    "    residual_coefficient: float = 0.0,\n",
    "    output_coefficient: float = 1.0,\n",
    "    use_diffusion_noise: bool = True,\n",
    "    memory: bool = False,\n",
    "    freeze_encoders: bool = True,\n",
    "    n_epochs: int = 2000,\n",
    "    patience: int = -1,\n",
    "    warmup_epochs: int = 500,\n",
    "    lr_drop_after: float = 0.66,\n",
    "    device: str = \"cpu\",\n",
    "    results_dir: str = \"results\",\n",
    "    random_state: int = 42,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    np.random.seed(random_state)\n",
    "    torch.manual_seed(random_state)\n",
    "\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    df_dict = {\n",
    "        \"n_nodes\": n_nodes,\n",
    "        \"density\": density,\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"n_iters\": n_iters,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"use_batchnorm\": use_batchnorm,\n",
    "        \"residual_coefficient\": residual_coefficient,\n",
    "        \"output_coefficient\": output_coefficient,\n",
    "        \"use_diffusion_noise\": use_diffusion_noise,\n",
    "        \"memory\": memory,\n",
    "        \"freeze_encoders\": freeze_encoders,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"patience\": patience,\n",
    "        \"warmup_epochs\": warmup_epochs,\n",
    "        \"lr_drop_after\": lr_drop_after,\n",
    "        \"device\": device,\n",
    "        \"random_state\": random_state,\n",
    "    }\n",
    "    uuid = str(abs(hash(tuple(df_dict.values()))))[:16]\n",
    "    log_file_name = f\"results_{uuid}.csv\"\n",
    "    if os.path.exists(os.path.join(results_dir, log_file_name)):\n",
    "        logger.warning(\n",
    "            f\"Results file {log_file_name} already exists. Skipping this run.\"\n",
    "        )\n",
    "        return None, None\n",
    "\n",
    "    # Generate the graph\n",
    "    graph = generate_graph(n_nodes=n_nodes, density=density, random_state=random_state)\n",
    "\n",
    "    # Generate the classification dataset\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = get_classification_data(\n",
    "        dataset_name\n",
    "    )\n",
    "\n",
    "    # Create the DataLoader instances\n",
    "    batch_size = 256\n",
    "    train_loader = get_dataloader(X_train, y_train, batch_size=batch_size)\n",
    "    val_loader = get_dataloader(X_val, y_val, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = get_dataloader(X_test, y_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    encoder_weights = os.path.join(\n",
    "        \"enc_dec_weights\", f\"{dataset_name}_encoder_weights.pth\"\n",
    "    )\n",
    "    if not os.path.exists(encoder_weights):\n",
    "        raise FileNotFoundError(f\"Pretrained weights not found for {dataset_name}.\")\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    encoder_model = FeedForwardNeuralNetwork(\n",
    "        in_features=X_train.shape[1],\n",
    "        out_features=n_classes,\n",
    "        hidden_dims=[],\n",
    "        dropout_rate=0.0,\n",
    "        use_batchnorm=False,\n",
    "    )\n",
    "    encoder_model.load_state_dict(torch.load(encoder_weights))\n",
    "    encoder_model.to(device)\n",
    "\n",
    "    # Create the model\n",
    "    model = MolComNetwork(\n",
    "        graph,\n",
    "        n_features=X_train.shape[1],\n",
    "        n_classes=n_classes,\n",
    "        n_iters=n_iters,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_batchnorm=use_batchnorm,\n",
    "        hidden_dims=hidden_dims,\n",
    "        residual_coefficient=residual_coefficient,\n",
    "        output_coefficient=output_coefficient,\n",
    "        encoder=encoder_model,\n",
    "        use_diffusion_noise=use_diffusion_noise,\n",
    "        memory=memory,\n",
    "        freeze_encoders=freeze_encoders,\n",
    "    )\n",
    "\n",
    "    if patience is None or patience < 0:\n",
    "        patience = n_epochs\n",
    "\n",
    "    # Train the model\n",
    "    train_metrics, test_metrics = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        n_epochs=n_epochs,\n",
    "        patience=patience,\n",
    "        warmup_epochs=warmup_epochs,\n",
    "        lr_drop_after=lr_drop_after,\n",
    "        device=device,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # Log the results\n",
    "    best_epoch = np.argmin(train_metrics[\"val\"][\"loss\"])\n",
    "    df_dict[\"best_epoch\"] = best_epoch\n",
    "    for key in train_metrics[\"train\"]:\n",
    "        df_dict[f\"train_{key}\"] = train_metrics[\"train\"][key][best_epoch]\n",
    "    for key in train_metrics[\"val\"]:\n",
    "        df_dict[f\"val_{key}\"] = train_metrics[\"val\"][key][best_epoch]\n",
    "    for key in test_metrics:\n",
    "        df_dict[f\"test_{key}\"] = test_metrics[key]\n",
    "\n",
    "    pd.DataFrame([df_dict]).to_csv(\n",
    "        os.path.join(results_dir, log_file_name), index=False\n",
    "    )\n",
    "\n",
    "    return train_metrics, test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ray for parallel processing\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def single_run_remote(**kwargs):\n",
    "    return single_run(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments on Ideal and Diffusion Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the set of experiments\n",
    "param_lists = {\n",
    "    \"random_state\": [42 + i for i in range(30)],\n",
    "    \"n_nodes\": [4, 8, 16, 32, 48, 64],\n",
    "    \"dataset_name\": [\"breast_cancer\", \"iris\", \"wine\"],\n",
    "    \"n_iters\": [1, 2, 4, 8],\n",
    "    \"use_diffusion_noise\": [True, False],\n",
    "    \"results_dir\": [\"results_cls\"],\n",
    "}\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "experiment_params = list(\n",
    "    dict(zip(param_lists.keys(), values)) for values in product(*param_lists.values())\n",
    ")\n",
    "\n",
    "# Shuffle the parameters\n",
    "shuffle(experiment_params)\n",
    "print(f\"Number of experiments: {len(experiment_params)}. Time: {datetime.now()}\")\n",
    "\n",
    "# Run the experiments in parallel\n",
    "results = []\n",
    "for params in experiment_params:\n",
    "    results.append(\n",
    "        single_run_remote.remote(\n",
    "            **params,\n",
    "            device=\"cpu\",\n",
    "        )\n",
    "    )\n",
    "results = ray.get(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments on Pure Relay Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the set of experiments\n",
    "param_lists = {\n",
    "    \"random_state\": [42 + i for i in range(30)],\n",
    "    \"n_nodes\": [4, 8, 16, 32, 48, 64],\n",
    "    \"dataset_name\": [\"breast_cancer\", \"iris\", \"wine\"],\n",
    "    \"n_iters\": [1, 2, 4, 8],\n",
    "    \"residual_coefficient\": [1.0],\n",
    "    \"output_coefficient\": [0.0],\n",
    "    \"results_dir\": [\"results_cls_relay\"],\n",
    "}\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "experiment_params = list(\n",
    "    dict(zip(param_lists.keys(), values)) for values in product(*param_lists.values())\n",
    ")\n",
    "\n",
    "# Shuffle the parameters\n",
    "shuffle(experiment_params)\n",
    "print(f\"Number of experiments: {len(experiment_params)}. Time: {datetime.now()}\")\n",
    "\n",
    "# Run the experiments in parallel\n",
    "results = []\n",
    "for params in experiment_params:\n",
    "    results.append(\n",
    "        single_run_remote.remote(\n",
    "            **params,\n",
    "            device=\"cpu\",\n",
    "        )\n",
    "    )\n",
    "results = ray.get(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect and Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(path, csv_file=None) -> pd.DataFrame:\n",
    "    if type(path) != list:\n",
    "        path = [path]\n",
    "    dfs = []\n",
    "    for p in path:\n",
    "        for file in tqdm(glob.glob(os.path.join(p, \"*.csv\"))):\n",
    "            dfs.append(pd.read_csv(file))\n",
    "    if len(dfs) == 0:\n",
    "        raise ValueError(f\"No files found in {path}\")\n",
    "    results_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Drop rows with freeze_encoders is False\n",
    "    if \"freeze_encoders\" in results_df.columns:\n",
    "        results_df = results_df[results_df[\"freeze_encoders\"] == True]\n",
    "\n",
    "    # Delete columns with constant values\n",
    "    results_df = results_df.loc[:, results_df.apply(pd.Series.nunique) != 1]\n",
    "\n",
    "    # Delete columns with loss or acc different from test\n",
    "    results_df = results_df.loc[:, ~results_df.columns.str.contains(\"train|val\")]\n",
    "\n",
    "    if csv_file is not None:\n",
    "        # Save the dataframe to a CSV file\n",
    "        results_df.to_csv(csv_file, index=False)\n",
    "        print(f\"Results saved to {csv_file}\")\n",
    "    return results_df\n",
    "\n",
    "\n",
    "@cache\n",
    "def load_baseline() -> pd.DataFrame:\n",
    "    df = load_results(\"results_cls_relay\")\n",
    "    # Filter only numerical columns\n",
    "    _df = df.select_dtypes(include=[np.number])\n",
    "    _df[\"dataset_name\"] = df[\"dataset_name\"]\n",
    "    # Average every numerical column grouping by dataset_name\n",
    "    df = _df.groupby([\"dataset_name\"]).mean().reset_index()\n",
    "    df.drop(\n",
    "        columns=[\"n_nodes\", \"n_iters\", \"random_state\", \"test_loss\", \"test_acc_avg\"],\n",
    "        inplace=True,\n",
    "    )\n",
    "    df.rename(\n",
    "        columns={\"dataset_name\": \"dataset\", \"test_acc_last\": \"acc_nl\"}, inplace=True\n",
    "    )\n",
    "\n",
    "    df_clf_baseline = pd.read_csv(\n",
    "        os.path.join(\"enc_dec_weights\", \"enc_dec_results.csv\")\n",
    "    )\n",
    "    # Join dataframe on dataset\n",
    "    df_clf_baseline = df_clf_baseline.merge(df, on=\"dataset\", how=\"right\")\n",
    "    return df_clf_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline = load_baseline()\n",
    "df_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_results([\"results_cls\"], csv_file=os.path.join(\"results_cls.csv\"))\n",
    "df_res = df[df[\"use_diffusion_noise\"] == True]\n",
    "df_res_nn = df[df[\"use_diffusion_noise\"] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_exp(ax, dataset, df, df_baseline, axes):\n",
    "    subset = df[df[\"dataset_name\"] == dataset]\n",
    "    n_iter = subset[\"n_iters\"].unique()[0]\n",
    "    sns.lineplot(\n",
    "        data=subset,\n",
    "        x=\"n_nodes\",\n",
    "        y=\"test_acc_last\",\n",
    "        hue=\"n_iters\",\n",
    "        ax=ax,\n",
    "        palette=\"tab10\",\n",
    "        marker=\"o\",\n",
    "    )\n",
    "\n",
    "    # Look up random‐guess and encoder–decoder acc from data_df\n",
    "    row = df_baseline.loc[df_baseline[\"dataset\"] == dataset].iloc[0]\n",
    "    rand_guess = 1.0 / row[\"n_classes\"]\n",
    "    enc_dec_acc = row[\"acc\"]\n",
    "    no_learning_acc = row[\"acc_nl\"]\n",
    "\n",
    "    # Draw dashed lines; only label them on the first subplot\n",
    "    if ax is axes[0]:\n",
    "        ax.axhline(enc_dec_acc, linestyle=\"--\", color=\"black\", label=\"Opt\")\n",
    "        ax.axhline(no_learning_acc, linestyle=\"--\", color=\"gray\", label=\"No Learning\")\n",
    "    else:\n",
    "        ax.axhline(enc_dec_acc, linestyle=\"--\", color=\"black\")\n",
    "        ax.axhline(no_learning_acc, linestyle=\"--\", color=\"gray\")\n",
    "\n",
    "    # Formatting\n",
    "    ax.get_legend().remove()\n",
    "    ax.set_ylim(rand_guess - 0.01, 1.01)\n",
    "    ax.set_xlim(df[\"n_nodes\"].min(), df[\"n_nodes\"].max())\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "\n",
    "def plot_results(df_res: pd.DataFrame, df_res_nn: pd.DataFrame, output_fig: str = None):\n",
    "    num_datasets = len(df_res[\"dataset_name\"].unique())\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=num_datasets,\n",
    "        ncols=2,\n",
    "        figsize=(6.3, 2 * num_datasets),\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "    )\n",
    "    fig.tight_layout()\n",
    "\n",
    "    for i, df in enumerate([df_res_nn, df_res]):\n",
    "        for j, dataset in enumerate(sorted(df[\"dataset_name\"].unique())):\n",
    "            plot_single_exp(axes[j, i], dataset, df, df_baseline, axes)\n",
    "\n",
    "    for j, dataset in enumerate(sorted(df_res[\"dataset_name\"].unique())):\n",
    "        axes[j, 0].set_ylabel(\n",
    "            f\"Test Accuracy\\non {' '.join(dataset.split('_')).title()}\"\n",
    "        )\n",
    "\n",
    "    for i in range(2):\n",
    "        axes[-1, i].set_xlabel(\"Number of Nodes\")\n",
    "\n",
    "    axes[0, 0].set_title(\"Ideal Channel\")\n",
    "    axes[0, 1].set_title(\"Diffusion Channel\")\n",
    "\n",
    "    # Build a global legend (includes both the line‐plot handles and our dashed‐line handles)\n",
    "    handles, labels = axes[-1, -1].get_legend_handles_labels()\n",
    "\n",
    "    handles, labels = axes[-1, -1].get_legend_handles_labels()\n",
    "\n",
    "    # Create explicit dashed‐line handles\n",
    "    opt_handle = Line2D([0], [0], color=\"black\", linestyle=\"--\", label=\"Optimal\")\n",
    "    no_learn_handle = Line2D([0], [0], color=\"gray\", linestyle=\"--\", label=\"Relay\")\n",
    "\n",
    "    # Append them\n",
    "    handles.extend([opt_handle, no_learn_handle])\n",
    "    labels.extend([\"Optimal\", \"Relay\"])\n",
    "\n",
    "    fig.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        loc=\"upper right\",\n",
    "        bbox_to_anchor=(0.94, -0.03),\n",
    "        title=\"Iterations\",\n",
    "        ncol=6,\n",
    "    )\n",
    "    if output_fig is not None:\n",
    "        plt.savefig(output_fig, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(df_res, df_res_nn, output_fig=os.path.join(\"results_cls.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
