{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🦠 Emergent Molecular Communication\n",
    "\n",
    "A preliminary study of emergent molecular communication protocols learned by graph-based agents in a diffusion channel environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Imports](#introduction)\n",
    "- [Utils](#utils)\n",
    "  - [Data](#data)\n",
    "  - [Neural Networks](#neural-networks)\n",
    "  - [Communication Model](#communication-model)\n",
    "  - [Train the Communication Model](#train-the-communication-model)\n",
    "- [Running the Experiments](#running-the-experiments)\n",
    "  - [Experiments on Ideal and Diffusion Channel](#experiments-on-ideal-and-diffusion-channel)\n",
    "  - [Experiments on Pure Relay Nodes](#experiments-on-pure-relay-nodes)\n",
    "- [Collect and Visualize Results](#collect-and-visualize-results)\n",
    "- [RL Environment](#rl-environment)\n",
    "- [RL Agent](#rl-agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x257cfb33710>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym # new import \n",
    "from gymnasium import spaces # new import \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  \n",
    "from typing import Optional, Tuple, Union, Dict, List, Any # Used older version of python (3.8), needed to import some type annotations\n",
    "\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "import logging\n",
    "from logging import getLogger\n",
    "import math\n",
    "import random\n",
    "from random import shuffle\n",
    "from itertools import product\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from functools import cache\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from scipy.special import erfc\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import ray\n",
    "from stable_baselines3 import PPO # new import \n",
    "from stable_baselines3.common.env_util import make_vec_env # new import \n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, BaseCallback # new import \n",
    "\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(\n",
    "    n_nodes: int, density: float, random_state: Optional[int] = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate random (x, y) positions for nodes in a graph.\n",
    "\n",
    "    The nodes are uniformly distributed in a square region.\n",
    "    The side length of the square is computed based on the given density,\n",
    "    so that the area of the square is n_nodes/density.\n",
    "\n",
    "    Parameters:\n",
    "        n_nodes (int): Number of nodes.\n",
    "        density (float): Average number of nodes per unit square.\n",
    "        random_state (int): Seed for the random number generator.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of shape (n_nodes, 2) with the (x, y) positions.\n",
    "    \"\"\"\n",
    "    # Initialize the random number generator with the provided seed.\n",
    "    if random_state is not None:\n",
    "        rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # Compute the side length of the square domain.\n",
    "    side_length = np.sqrt(n_nodes / density)\n",
    "\n",
    "    # Generate positions uniformly in the square [0, side_length] x [0, side_length].\n",
    "    positions = rng.uniform(low=0, high=side_length, size=(n_nodes, 2))\n",
    "\n",
    "    return positions\n",
    "\n",
    "\n",
    "def get_sender_receiver(graph: np.ndarray) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Return the index of the sender and receiver nodes. The sender and the receiver are the nodes\n",
    "    that are furthest apart in the graph.\n",
    "\n",
    "    Parameters:\n",
    "        graph (np.ndarray): An array of shape (n_nodes, 2) with the (x, y) positions of the nodes.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[int, int]: The indices of the sender and receiver nodes.\n",
    "    \"\"\"\n",
    "    # Compute the pairwise distances between all nodes.\n",
    "    distances = np.linalg.norm(graph[:, None] - graph, axis=-1)\n",
    "\n",
    "    # Find the indices of the sender and receiver nodes.\n",
    "    sender, receiver = np.unravel_index(np.argmax(distances), distances.shape)\n",
    "\n",
    "    return sender.item(), receiver.item()\n",
    "\n",
    "\n",
    "def get_distance_matrix(graph: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the pairwise distance matrix between all nodes in the graph.\n",
    "\n",
    "    Parameters:\n",
    "        graph (np.ndarray): An array of shape (n_nodes, 2) with the (x, y) positions of the nodes.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A distance matrix of shape (n_nodes, n_nodes) where the entry (i, j) is the\n",
    "            Euclidean distance between nodes i and j.\n",
    "    \"\"\"\n",
    "    # Compute the pairwise distances between all nodes.\n",
    "    distances = np.linalg.norm(graph[:, None] - graph, axis=-1)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "def plot_graph(graph: np.ndarray, ax=None):\n",
    "    \"\"\"\n",
    "    Plot the graph with nodes and edges.\n",
    "\n",
    "    Parameters:\n",
    "        graph (np.ndarray): An array of shape (n_nodes, 2) with the (x, y) positions of the nodes.\n",
    "        ax (plt.Axes): The axes where to plot the graph.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "\n",
    "    # Plot the nodes.\n",
    "    ax.plot(graph[:, 0], graph[:, 1], \"o\", label=\"Nodes\")\n",
    "    for i, (x, y) in enumerate(graph):\n",
    "        ax.text(x, y, f\"{i}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    # Plot the sender and the receiver nodes with a different color.\n",
    "    sender, receiver = get_sender_receiver(graph)\n",
    "    ax.plot(graph[sender, 0], graph[sender, 1], \"o\", color=\"green\", label=\"Sender\")\n",
    "    ax.plot(graph[receiver, 0], graph[receiver, 1], \"o\", color=\"red\", label=\"Receiver\")\n",
    "\n",
    "    ax.set_aspect(\"equal\", \"box\")\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_title(\"Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>n_samples</th>\n",
       "      <th>n_features</th>\n",
       "      <th>n_classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iris</td>\n",
       "      <td>150</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wine</td>\n",
       "      <td>178</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>breast_cancer</td>\n",
       "      <td>569</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name  n_samples  n_features  n_classes\n",
       "0           iris        150           4          3\n",
       "1           wine        178          13          3\n",
       "2  breast_cancer        569          30          2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASETS = [\n",
    "    \"iris\",\n",
    "    \"wine\",\n",
    "    \"breast_cancer\",\n",
    "]\n",
    "\n",
    "def get_classification_data(\n",
    "    dataset_name: str,\n",
    "    test_size: float = 0.2,\n",
    "    val_size: float = 0.1,\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a dataset from OpenML (UCI repository) and preprocess it:\n",
    "    - Detect numerical and categorical features\n",
    "    - Impute missing values\n",
    "    - One-hot encode categorical features\n",
    "    - Standardize all features\n",
    "    - Encode target labels\n",
    "    - Split into train/validation/test sets\n",
    "\n",
    "    Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test as numpy arrays\n",
    "    \"\"\"\n",
    "    # Check if the dataset is in the list of available datasets\n",
    "    if dataset_name not in DATASETS:\n",
    "        raise ValueError(\n",
    "            f\"Dataset {dataset_name} is not available. Please choose from {DATASETS}.\"\n",
    "        )\n",
    "\n",
    "    # Check if the dataset file exists\n",
    "    dataset_path = os.path.join(\"..\", \"data\", f\"{dataset_name}.npz\")\n",
    "    if os.path.exists(dataset_path):\n",
    "        with np.load(dataset_path, allow_pickle=False) as data:\n",
    "            X_train = data[\"X_train\"]\n",
    "            X_val = data[\"X_val\"]\n",
    "            X_test = data[\"X_test\"]\n",
    "            y_train = data[\"y_train\"]\n",
    "            y_val = data[\"y_val\"]\n",
    "            y_test = data[\"y_test\"]\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "    # fetch dataset\n",
    "    ids = {\n",
    "        \"iris\": 53,\n",
    "        \"wine\": 109,\n",
    "        \"breast_cancer\": 17,\n",
    "    }\n",
    "    if dataset_name not in DATASETS:\n",
    "        raise ValueError(\n",
    "            f\"Dataset {dataset_name} is not available. Please choose from {DATASETS}.\"\n",
    "        )\n",
    "\n",
    "    ds = fetch_ucirepo(id=ids[dataset_name])\n",
    "    X = ds.data.features\n",
    "    y = ds.data.targets\n",
    "\n",
    "    # Identify feature types\n",
    "    numeric_features = X.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    categorical_features = [col for col in X.columns if col not in numeric_features]\n",
    "\n",
    "    # Pipelines for numerical and categorical features\n",
    "    numeric_pipeline = Pipeline(\n",
    "        [(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    "    )\n",
    "    categorical_pipeline = Pipeline(\n",
    "        [\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        [\n",
    "            (\"num\", numeric_pipeline, numeric_features),\n",
    "            (\"cat\", categorical_pipeline, categorical_features),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Encode target labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=test_size, random_state=random_state, stratify=y_encoded\n",
    "    )\n",
    "    # Adjust validation size relative to the remaining data\n",
    "    val_relative_size = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp,\n",
    "        y_temp,\n",
    "        test_size=val_relative_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y_temp,\n",
    "    )\n",
    "\n",
    "    # Fit the preprocessor on the training data and transform all sets\n",
    "    if dataset_name in ids:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            X_train_processed = preprocessor.fit_transform(X_train)\n",
    "            X_val_processed = preprocessor.transform(X_val)\n",
    "            X_test_processed = preprocessor.transform(X_test)\n",
    "    else:\n",
    "        X_train_processed = X_train\n",
    "        X_val_processed = X_val\n",
    "        X_test_processed = X_test\n",
    "\n",
    "    # Save the dataset as a numpy array\n",
    "    os.makedirs(os.path.join(\"..\", \"data\"), exist_ok=True)\n",
    "    np.savez_compressed(\n",
    "        os.path.join(\"..\", \"data\", f\"{dataset_name}.npz\"),\n",
    "        X_train=X_train_processed,\n",
    "        X_val=X_val_processed,\n",
    "        X_test=X_test_processed,\n",
    "        y_train=y_train,\n",
    "        y_val=y_val,\n",
    "        y_test=y_test,\n",
    "        allow_pickle=False,\n",
    "    )\n",
    "\n",
    "    return X_train_processed, X_val_processed, X_test_processed, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "data_stats_dict = {\n",
    "    \"name\": [],\n",
    "    \"n_samples\": [],\n",
    "    \"n_features\": [],\n",
    "    \"n_classes\": [],\n",
    "}\n",
    "for dataset in DATASETS:\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = get_classification_data(dataset)\n",
    "    data_stats_dict[\"name\"].append(dataset)\n",
    "    data_stats_dict[\"n_samples\"].append(\n",
    "        X_train.shape[0] + X_val.shape[0] + X_test.shape[0]\n",
    "    )\n",
    "    data_stats_dict[\"n_features\"].append(X_train.shape[1])\n",
    "    data_stats_dict[\"n_classes\"].append(len(np.unique(y_train)))\n",
    "data_stats_df = pd.DataFrame(data_stats_dict)\n",
    "data_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        hidden_dims: list = [4],\n",
    "        dropout_rate: float = 0.0,\n",
    "        use_batchnorm: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Feedforward Neural Network with ReLU activations and Dropout.\n",
    "\n",
    "        Parameters:\n",
    "            in_features (int): Dimensionality of the input features.\n",
    "            out_features (int): Dimensionality of the output features.\n",
    "            hidden_dims (list): List with the number of neurons for each hidden layer.\n",
    "            dropout_rate (float): Dropout probability.\n",
    "            use_batchnorm (bool): Whether to use Batch Normalization.\n",
    "            residual_coefficient (float): Coefficient for the residual connection.\n",
    "            output_coefficient (float): Coefficient for the output layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = in_features\n",
    "        # Build hidden layers\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_dim = h\n",
    "        layers.append(nn.Linear(prev_dim, out_features))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Parameters:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 20:30:02,723 - INFO - Encoder-decoder weights already exist for iris.\n",
      "2025-07-01 20:30:02,724 - INFO - Encoder-decoder weights already exist for wine.\n",
      "2025-07-01 20:30:02,726 - INFO - Encoder-decoder weights already exist for breast_cancer.\n"
     ]
    }
   ],
   "source": [
    "def train_encoder_decoder(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    n_features,\n",
    "    n_classes,\n",
    "    hidden_dims,\n",
    "    use_batchnorm,\n",
    "    lr=1e-3,\n",
    "    n_epochs=50,\n",
    "    patience=5,\n",
    "    lr_drop_after=0.5,\n",
    "    warmup_epochs=10,\n",
    "    l1_lambda=0.01,\n",
    "    batch_size=256,\n",
    "    device=\"cpu\",\n",
    "    weights_dir=\"enc_dec_weights\",\n",
    "    verbose=False,\n",
    "):\n",
    "    os.makedirs(weights_dir, exist_ok=True)\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Build the two networks\n",
    "    n_chem = n_classes\n",
    "    encoder = FeedForwardNeuralNetwork(\n",
    "        in_features=n_features,\n",
    "        out_features=n_chem,\n",
    "        hidden_dims=hidden_dims,\n",
    "        use_batchnorm=use_batchnorm,\n",
    "    ).to(device)\n",
    "    decoder = FeedForwardNeuralNetwork(\n",
    "        in_features=n_chem,\n",
    "        out_features=n_classes,\n",
    "        hidden_dims=hidden_dims,\n",
    "        use_batchnorm=use_batchnorm,\n",
    "    ).to(device)\n",
    "\n",
    "    # Joint optimizer & loss\n",
    "    optimizer = optim.Adam(\n",
    "        list(encoder.parameters()) + list(decoder.parameters()), lr=lr\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # DataLoaders\n",
    "    train_ds = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.long), # <--- Add dtype=torch.long\n",
    ")\n",
    "    val_ds = TensorDataset(\n",
    "    torch.tensor(X_val, dtype=torch.float32),\n",
    "    torch.tensor(y_val, dtype=torch.long),   # <--- Add dtype=torch.long\n",
    ")\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    best_enc = deepcopy(encoder.state_dict())\n",
    "    best_dec = deepcopy(decoder.state_dict())\n",
    "    no_imp = 0\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # — train epoch —\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        running = total = 0.0\n",
    "        for Xb, yb in train_loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            chem = encoder(Xb)\n",
    "            logits = decoder(chem)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "            # Add L1 regularization\n",
    "            l1_norm = sum(p.abs().sum() for p in encoder.parameters()) + sum(\n",
    "                p.abs().sum() for p in decoder.parameters()\n",
    "            )\n",
    "            loss += l1_lambda * l1_norm\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running += loss.item() * Xb.size(0)\n",
    "            total += Xb.size(0)\n",
    "\n",
    "        # — validation epoch —\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        vrun = vtot = 0.0\n",
    "        with torch.no_grad():\n",
    "            for Xb, yb in val_loader:\n",
    "                Xb, yb = Xb.to(device), yb.to(device)\n",
    "                vchem = encoder(Xb)\n",
    "                vlogits = decoder(vchem)\n",
    "                vrun += criterion(vlogits, yb).item() * Xb.size(0)\n",
    "                vtot += Xb.size(0)\n",
    "\n",
    "        train_loss = running / total\n",
    "        val_loss = vrun / vtot\n",
    "        val_acc = (vlogits.argmax(dim=1) == yb).float().mean().item()\n",
    "        if verbose:\n",
    "            logger.info(\n",
    "                f\"[EncDec] Epoch {epoch}: train={train_loss:.4f} val={val_loss:.4f} acc={val_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "        # early stop\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_enc = deepcopy(encoder.state_dict())\n",
    "            best_dec = deepcopy(decoder.state_dict())\n",
    "            no_imp = 0\n",
    "        else:\n",
    "            no_imp += 1\n",
    "            if no_imp >= patience and epoch > warmup_epochs:\n",
    "                if verbose:\n",
    "                    logger.warning(\"Encoder-decoder pretraining stopped early.\")\n",
    "                break\n",
    "\n",
    "        # learning rate drop\n",
    "        drop_epoch = math.ceil(n_epochs * lr_drop_after)\n",
    "        if epoch > warmup_epochs and epoch == drop_epoch:\n",
    "            for g in optimizer.param_groups:\n",
    "                g[\"lr\"] = lr / 10\n",
    "            if verbose:\n",
    "                logger.info(f\"Learning rate dropped to {lr / 10:.1e} at epoch {epoch}.\")\n",
    "\n",
    "    # save best\n",
    "    encoder.load_state_dict(best_enc)\n",
    "    decoder.load_state_dict(best_dec)\n",
    "    enc_path = os.path.join(weights_dir, f\"{dataset_name}_encoder_weights.pth\")\n",
    "    dec_path = os.path.join(weights_dir, f\"{dataset_name}_decoder_weights.pth\")\n",
    "    torch.save(encoder.state_dict(), enc_path)\n",
    "    torch.save(decoder.state_dict(), dec_path)\n",
    "    if verbose:\n",
    "        logger.debug(f\"Saved encoder → {enc_path}\")\n",
    "        logger.debug(f\"Saved decoder → {dec_path}\")\n",
    "\n",
    "    # test\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    correct, total = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in test_loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            tchem = encoder(Xb)\n",
    "            tlogits = decoder(tchem)\n",
    "            correct += (tlogits.argmax(dim=1) == yb).float().sum().item()\n",
    "            total += Xb.size(0)\n",
    "    test_acc = correct / total\n",
    "    return enc_path, dec_path, test_acc\n",
    "\n",
    "\n",
    "os.makedirs(\"enc_dec_weights\", exist_ok=True)\n",
    "res_csv_path = os.path.join(\"enc_dec_weights\", \"enc_dec_results.csv\")\n",
    "if os.path.exists(res_csv_path):\n",
    "    df = pd.read_csv(res_csv_path)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=[\"dataset\", \"n_features\", \"n_classes\", \"acc\"])\n",
    "    df.to_csv(res_csv_path, index=False)\n",
    "\n",
    "for dataset_name in DATASETS:\n",
    "    if dataset_name in [\"identity\", \"shift\", \"negate\"]:\n",
    "        continue\n",
    "    enc_path = os.path.join(\"enc_dec_weights\", f\"{dataset_name}_encoder_weights.pth\")\n",
    "    dec_path = os.path.join(\"enc_dec_weights\", f\"{dataset_name}_decoder_weights.pth\")\n",
    "    if not os.path.exists(enc_path) or not os.path.exists(dec_path):\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = get_classification_data(\n",
    "            dataset_name\n",
    "        )\n",
    "        n_features = X_train.shape[1]\n",
    "        n_classes = len(np.unique(y_train))\n",
    "        _, _, acc = train_encoder_decoder(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_val,\n",
    "            y_val,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            n_features,\n",
    "            n_classes,\n",
    "            hidden_dims=[],\n",
    "            use_batchnorm=False,\n",
    "            lr=1e-3,\n",
    "            n_epochs=10000,\n",
    "            patience=100,\n",
    "            batch_size=1024,\n",
    "            device=\"cpu\",\n",
    "            weights_dir=\"enc_dec_weights\",\n",
    "        )\n",
    "        logger.info(f\"Trained encoder-decoder on {dataset_name}. Acc: {acc:.4f}\")\n",
    "        df = pd.read_csv(res_csv_path)\n",
    "        new_row = pd.DataFrame(\n",
    "            {\n",
    "                \"dataset\": [dataset_name],\n",
    "                \"n_features\": [n_features],\n",
    "                \"n_classes\": [n_classes],\n",
    "                \"acc\": [acc],\n",
    "            }\n",
    "        )\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.to_csv(res_csv_path, index=False)\n",
    "    else:\n",
    "        logger.info(f\"Encoder-decoder weights already exist for {dataset_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communication Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_passing(\n",
    "    message: torch.Tensor,\n",
    "    distance: torch.Tensor,\n",
    "    phi: nn.ModuleList,\n",
    "    residual_coefficient: float = 0.0,\n",
    "    output_coefficient: float = 1.0,\n",
    "    use_diffusion_noise: bool = True,\n",
    "    memory: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Perform one message passing iteration, optionally incorporating memory.\n",
    "\n",
    "    Parameters:\n",
    "        message: Tensor of shape (B, n, d)  -- messages at current iteration\n",
    "        distance: Tensor of shape (n, n)     -- distance matrix between nodes\n",
    "        phi:     ModuleList of n mappings   -- each maps R^{d or 2d} -> R^d\n",
    "        residual_coefficient: float         -- coefficient for residual connection\n",
    "        output_coefficient: float          -- coefficient for output layer\n",
    "        use_diffusion_noise: bool          -- whether to add diffusion noise\n",
    "        memory:  bool                        -- whether to concatenate previous message\n",
    "\n",
    "    Returns:\n",
    "        new_message: Tensor of shape (B, n, d) -- updated messages\n",
    "    \"\"\"\n",
    "    B, N, D = message.shape\n",
    "\n",
    "    # Save previous messages if memory is on\n",
    "    prev_msg = message.clone() if memory else None\n",
    "\n",
    "    # Compute detection probabilities\n",
    "    p = torch.erfc(distance / 2.0)  # (n, n)\n",
    "    p_exp = p.unsqueeze(0).unsqueeze(-1).expand(B, N, N, D)  # (B,n,n,d)\n",
    "\n",
    "    # Channel noise\n",
    "    if use_diffusion_noise:\n",
    "        eps = torch.randn(B, N, N, D, device=message.device)\n",
    "        Delta = (p_exp * (1 + eps - eps * p_exp)).clip(min=0)  # (B,n,n,d)\n",
    "    else:\n",
    "        Delta = torch.ones_like(p_exp, device=message.device)\n",
    "\n",
    "    # Compute aggregated messages\n",
    "    aggregated = torch.einsum(\"bijk,bjk->bik\", Delta, message)  # (B,n,d)\n",
    "\n",
    "    # Compute new messages per node\n",
    "    new_message = torch.zeros_like(message, device=message.device)\n",
    "    for i in range(N):\n",
    "        if memory:\n",
    "            # concat along feature dim: [aggregated, previous]\n",
    "            inp = torch.cat([aggregated[:, i, :], prev_msg[:, i, :]], dim=-1)\n",
    "        else:\n",
    "            inp = aggregated[:, i, :]\n",
    "        new_message[:, i, :] = phi[i](inp)\n",
    "\n",
    "    # Apply residual connection\n",
    "    new_message = residual_coefficient * message + output_coefficient * new_message\n",
    "    return new_message\n",
    "\n",
    "\n",
    "class MolComNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph: np.ndarray,\n",
    "        n_features: int,\n",
    "        n_classes: int,\n",
    "        encoder: torch.nn.Module = None,\n",
    "        n_iters: int = 1,\n",
    "        hidden_dims: list = [4],\n",
    "        dropout_rate: float = 0.0,\n",
    "        use_batchnorm: bool = False,\n",
    "        residual_coefficient: float = 0.0,\n",
    "        output_coefficient: float = 1.0,\n",
    "        use_diffusion_noise: bool = True,\n",
    "        memory: bool = False,\n",
    "        freeze_encoders: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.graph = graph\n",
    "        self.graph_size = len(graph)\n",
    "        self.distances = torch.tensor(get_distance_matrix(graph), dtype=torch.float32)\n",
    "        self.sender, self.receiver = get_sender_receiver(graph)\n",
    "        self.n_iters = n_iters\n",
    "        self.residual_coefficient = residual_coefficient\n",
    "        self.output_coefficient = output_coefficient\n",
    "        self.n_classes = n_classes\n",
    "        self.use_diffusion_noise = use_diffusion_noise\n",
    "        self.memory = memory\n",
    "\n",
    "        # Encoder unchanged\n",
    "        self.encoder = encoder\n",
    "        if freeze_encoders and encoder is not None:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # φ‐networks: double in_features if memory=True\n",
    "        phi_in = 2 * n_classes if memory else n_classes\n",
    "        self.phi = nn.ModuleList(\n",
    "            [\n",
    "                FeedForwardNeuralNetwork(\n",
    "                    in_features=phi_in,\n",
    "                    out_features=n_classes,\n",
    "                    hidden_dims=hidden_dims,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    use_batchnorm=use_batchnorm,\n",
    "                )\n",
    "                for _ in range(self.graph_size)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B = x.size(0)\n",
    "        self.distances = self.distances.to(x.device)\n",
    "        assert (\n",
    "            x.shape[1] == self.n_features\n",
    "        ), f\"Expected {self.n_features} features, got {x.shape[1]}\"\n",
    "\n",
    "        # Initialize messages: only sender nodes get the encoded signal\n",
    "        msg = torch.zeros(B, self.graph_size, self.n_classes, device=x.device)\n",
    "        msg[:, self.sender, :] = self.encoder(x) if self.encoder is not None else x\n",
    "        msg = torch.relu(msg)\n",
    "        out = torch.zeros(B, self.n_classes, self.n_iters, device=x.device)\n",
    "\n",
    "        # Iterative message passing\n",
    "        for i in range(self.n_iters):\n",
    "            msg = message_passing(\n",
    "                msg,\n",
    "                self.distances,\n",
    "                self.phi,\n",
    "                residual_coefficient=self.residual_coefficient,\n",
    "                output_coefficient=self.output_coefficient,\n",
    "                memory=self.memory,\n",
    "                use_diffusion_noise=self.use_diffusion_noise,\n",
    "            )\n",
    "            msg = torch.relu(msg)\n",
    "            # Save the output of the last iteration\n",
    "            out[:, :, i] = msg[:, self.receiver, :]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Communication Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iter_metrics(\n",
    "    outputs: torch.Tensor, targets: torch.Tensor, k: int\n",
    ") -> dict[str, float | list[float]]:\n",
    "    \"\"\"\n",
    "    Returns a dict with:\n",
    "      - acc_last       float\n",
    "      - acc_avg        float\n",
    "      - mean_first     float\n",
    "      - conv_rate      float\n",
    "      - stability      float\n",
    "      - acc_per_iter   list[float]\n",
    "    \"\"\"\n",
    "    B, C, T = outputs.shape\n",
    "\n",
    "    preds = outputs.argmax(dim=1)  # (B, T)\n",
    "    correct = preds.eq(targets.unsqueeze(1))  # (B, T)\n",
    "    acc_per_iter = correct.float().mean(dim=0).tolist()\n",
    "\n",
    "    acc_last = acc_per_iter[-1]\n",
    "\n",
    "    probs = F.softmax(outputs, dim=1)  # (B, C, T)\n",
    "    acc_avg = (probs.mean(dim=2).argmax(dim=1) == targets).float().mean().item()\n",
    "\n",
    "    idx = torch.arange(T, device=outputs.device).unsqueeze(0).expand(B, T)\n",
    "    first_idx = torch.where(correct, idx, torch.full_like(idx, T)).min(dim=1)[0] + 1\n",
    "    valid = first_idx <= T\n",
    "    mean_first = first_idx[valid].float().mean().item() if valid.any() else float(\"nan\")\n",
    "\n",
    "    k_clamped = max(1, min(k, T))\n",
    "    stable = (preds[:, k_clamped - 1].unsqueeze(1) == preds[:, k_clamped - 1 :]).all(\n",
    "        dim=1\n",
    "    )\n",
    "    conv_rate = stable.float().mean().item()\n",
    "\n",
    "    flips = (preds[:, 1:] != preds[:, :-1]).float().sum(dim=1)\n",
    "    stability = 1 - (flips / (T - 1)).mean().item()\n",
    "\n",
    "    return {\n",
    "        \"acc_last\": acc_last,\n",
    "        \"acc_avg\": acc_avg,\n",
    "        \"mean_first\": mean_first,\n",
    "        \"conv_rate\": conv_rate,\n",
    "        \"stability\": stability,\n",
    "        \"acc_per_iter\": acc_per_iter,\n",
    "    }\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, k):\n",
    "    model.train()\n",
    "    meters = {\n",
    "        \"loss\": 0.0,\n",
    "        \"acc_last\": 0.0,\n",
    "        \"acc_avg\": 0.0,\n",
    "        \"mean_first\": 0.0,\n",
    "        \"conv_rate\": 0.0,\n",
    "        \"stability\": 0.0,\n",
    "        \"acc_per_iter\": None,  # will become list\n",
    "        \"count\": 0,\n",
    "    }\n",
    "\n",
    "    for X, y in loader:\n",
    "        B = X.size(0)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(X)  # (B, C, T)\n",
    "        T = out.size(2)\n",
    "\n",
    "        loss = sum(criterion(out[:, :, t], y) for t in range(T)) / T\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        metrics = compute_iter_metrics(out, y, k)\n",
    "\n",
    "        meters[\"loss\"] += loss.item() * B\n",
    "        meters[\"acc_last\"] += metrics[\"acc_last\"] * B\n",
    "        meters[\"acc_avg\"] += metrics[\"acc_avg\"] * B\n",
    "        meters[\"mean_first\"] += metrics[\"mean_first\"] * B\n",
    "        meters[\"conv_rate\"] += metrics[\"conv_rate\"] * B\n",
    "        meters[\"stability\"] += metrics[\"stability\"] * B\n",
    "\n",
    "        if meters[\"acc_per_iter\"] is None:\n",
    "            meters[\"acc_per_iter\"] = torch.zeros(T, dtype=torch.float64)\n",
    "        meters[\"acc_per_iter\"] += (\n",
    "            torch.tensor(metrics[\"acc_per_iter\"], dtype=torch.float64) * B\n",
    "        )\n",
    "\n",
    "        meters[\"count\"] += B\n",
    "\n",
    "    # normalize\n",
    "    N = meters[\"count\"]\n",
    "    epoch = {\n",
    "        k: (v / N if k != \"acc_per_iter\" else (meters[\"acc_per_iter\"] / N).tolist())\n",
    "        for k, v in meters.items()\n",
    "        if k != \"count\"\n",
    "    }\n",
    "    return epoch\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(model, loader, criterion, device, k):\n",
    "    model.eval()\n",
    "    meters = {\n",
    "        \"loss\": 0.0,\n",
    "        \"acc_last\": 0.0,\n",
    "        \"acc_avg\": 0.0,\n",
    "        \"mean_first\": 0.0,\n",
    "        \"conv_rate\": 0.0,\n",
    "        \"stability\": 0.0,\n",
    "        \"acc_per_iter\": None,\n",
    "        \"count\": 0,\n",
    "    }\n",
    "\n",
    "    for X, y in loader:\n",
    "        B = X.size(0)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        out = model(X)\n",
    "        T = out.size(2)\n",
    "        loss = sum(criterion(out[:, :, t], y) for t in range(T)) / T\n",
    "\n",
    "        metrics = compute_iter_metrics(out, y, k)\n",
    "\n",
    "        meters[\"loss\"] += loss.item() * B\n",
    "        meters[\"acc_last\"] += metrics[\"acc_last\"] * B\n",
    "        meters[\"acc_avg\"] += metrics[\"acc_avg\"] * B\n",
    "        meters[\"mean_first\"] += metrics[\"mean_first\"] * B\n",
    "        meters[\"conv_rate\"] += metrics[\"conv_rate\"] * B\n",
    "        meters[\"stability\"] += metrics[\"stability\"] * B\n",
    "\n",
    "        if meters[\"acc_per_iter\"] is None:\n",
    "            meters[\"acc_per_iter\"] = torch.zeros(T, dtype=torch.float64)\n",
    "        meters[\"acc_per_iter\"] += (\n",
    "            torch.tensor(metrics[\"acc_per_iter\"], dtype=torch.float64) * B\n",
    "        )\n",
    "\n",
    "        meters[\"count\"] += B\n",
    "\n",
    "    N = meters[\"count\"]\n",
    "    epoch = {\n",
    "        k: (v / N if k != \"acc_per_iter\" else (meters[\"acc_per_iter\"] / N).tolist())\n",
    "        for k, v in meters.items()\n",
    "        if k != \"count\"\n",
    "    }\n",
    "    return epoch\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    n_epochs: int = 100,\n",
    "    patience: int = 10,\n",
    "    warmup_epochs: int = 25,\n",
    "    lr_drop_after: float = 0.5,\n",
    "    lr: float = 1e-3,\n",
    "    device: str = \"cpu\",\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    warmup_epochs = min(warmup_epochs, n_epochs)\n",
    "    lr_drop_epoch = math.ceil(n_epochs * lr_drop_after)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        device = torch.device(device)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_state = deepcopy(model.state_dict())\n",
    "        no_improve = 0\n",
    "\n",
    "        # Initialize history\n",
    "        history: dict[str, dict[str, list]] = {\n",
    "            \"train\": {\n",
    "                k: []\n",
    "                for k in [\n",
    "                    \"loss\",\n",
    "                    \"acc_last\",\n",
    "                    \"acc_avg\",\n",
    "                    \"mean_first\",\n",
    "                    \"conv_rate\",\n",
    "                    \"stability\",\n",
    "                    \"acc_per_iter\",\n",
    "                ]\n",
    "            },\n",
    "            \"val\": {\n",
    "                k: []\n",
    "                for k in [\n",
    "                    \"loss\",\n",
    "                    \"acc_last\",\n",
    "                    \"acc_avg\",\n",
    "                    \"mean_first\",\n",
    "                    \"conv_rate\",\n",
    "                    \"stability\",\n",
    "                    \"acc_per_iter\",\n",
    "                ]\n",
    "            },\n",
    "        }\n",
    "\n",
    "        k = int(math.sqrt(model.graph_size))\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            train_metrics = train_one_epoch(\n",
    "                model, train_loader, optimizer, criterion, device, k\n",
    "            )\n",
    "            val_metrics = eval_one_epoch(model, val_loader, criterion, device, k)\n",
    "\n",
    "            # Log and store\n",
    "            for phase, metrics in [(\"train\", train_metrics), (\"val\", val_metrics)]:\n",
    "                for name, val in metrics.items():\n",
    "                    history[phase][name].append(val)\n",
    "                # Logging with loguru\n",
    "            if verbose:\n",
    "                logger.info(\n",
    "                    f\"Epoch {epoch:03} | \"\n",
    "                    f\"Train L={train_metrics['loss']:.4f}, last@={train_metrics['acc_last']:.3f}, \"\n",
    "                    f\"avg@={train_metrics['acc_avg']:.3f}, conv@k={train_metrics['conv_rate']:.3f}, \"\n",
    "                    f\"stab={train_metrics['stability']:.3f}, \"\n",
    "                    f\"mean_first={train_metrics['mean_first']:.3f} | \"\n",
    "                    f\"Val   L={val_metrics['loss']:.4f}, last@={val_metrics['acc_last']:.3f}, \"\n",
    "                    f\"avg@={val_metrics['acc_avg']:.3f}, conv@k={val_metrics['conv_rate']:.3f}, \"\n",
    "                    f\"stab={val_metrics['stability']:.3f}, \"\n",
    "                    f\"mean_first={val_metrics['mean_first']:.3f}\"\n",
    "                )\n",
    "\n",
    "            # Early stopping\n",
    "            if val_metrics[\"loss\"] < best_val_loss:\n",
    "                best_val_loss = val_metrics[\"loss\"]\n",
    "                best_state = deepcopy(model.state_dict())\n",
    "                no_improve = 0\n",
    "                if verbose:\n",
    "                    logger.debug(\n",
    "                        f\"Epoch {epoch}: New best val loss: {best_val_loss:.4f}\"\n",
    "                    )\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= patience and epoch > warmup_epochs:\n",
    "                    if verbose:\n",
    "                        logger.info(\n",
    "                            f\"Early stopping at epoch {epoch} with patience {patience}.\"\n",
    "                        )\n",
    "                    break\n",
    "\n",
    "            # Learning rate drop\n",
    "            if epoch > warmup_epochs and epoch == lr_drop_epoch:\n",
    "                for g in optimizer.param_groups:\n",
    "                    g[\"lr\"] = lr / 10\n",
    "                if verbose:\n",
    "                    logger.info(\n",
    "                        f\"Learning rate dropped to {lr / 10:.1e} at epoch {epoch}.\"\n",
    "                    )\n",
    "\n",
    "        # Restore best and test\n",
    "        model.load_state_dict(best_state)\n",
    "        test_metrics = eval_one_epoch(model, test_loader, criterion, device, k)\n",
    "        if verbose:\n",
    "            logger.info(\n",
    "                f\"Test L={test_metrics['loss']:.4f}, last@={test_metrics['acc_last']:.3f}, \"\n",
    "                f\"avg@={test_metrics['acc_avg']:.3f}, conv@k={test_metrics['conv_rate']:.3f}, \"\n",
    "                f\"stab={test_metrics['stability']:.3f}, \"\n",
    "                f\"mean_first={test_metrics['mean_first']:.3f}\"\n",
    "            )\n",
    "\n",
    "    return history, test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(\n",
    "    X: np.ndarray, y: np.ndarray, batch_size: int = 1, shuffle: bool = True\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Create a DataLoader from the features and targets.\n",
    "\n",
    "    Parameters:\n",
    "        X (np.ndarray): Features.\n",
    "        y (np.ndarray): Targets.\n",
    "        batch_size (int): Batch size.\n",
    "        shuffle (bool): Whether to shuffle the data.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: PyTorch DataLoader.\n",
    "    \"\"\"\n",
    "    dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X, dtype=torch.float32), torch.tensor(y)\n",
    "    )\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_run(\n",
    "    n_nodes: int = 10,\n",
    "    density: float = 1,\n",
    "    dataset_name: str = \"iris\",\n",
    "    n_iters: int = 10,\n",
    "    hidden_dims: list = [4],\n",
    "    dropout_rate: float = 0.0,\n",
    "    use_batchnorm: bool = False,\n",
    "    residual_coefficient: float = 0.0,\n",
    "    output_coefficient: float = 1.0,\n",
    "    use_diffusion_noise: bool = True,\n",
    "    memory: bool = False,\n",
    "    freeze_encoders: bool = True,\n",
    "    n_epochs: int = 2000,\n",
    "    patience: int = -1,\n",
    "    warmup_epochs: int = 500,\n",
    "    lr_drop_after: float = 0.66,\n",
    "    device: str = \"cpu\",\n",
    "    results_dir: str = \"results\",\n",
    "    random_state: int = 42,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    np.random.seed(random_state)\n",
    "    torch.manual_seed(random_state)\n",
    "\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    df_dict = {\n",
    "        \"n_nodes\": n_nodes,\n",
    "        \"density\": density,\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"n_iters\": n_iters,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"use_batchnorm\": use_batchnorm,\n",
    "        \"residual_coefficient\": residual_coefficient,\n",
    "        \"output_coefficient\": output_coefficient,\n",
    "        \"use_diffusion_noise\": use_diffusion_noise,\n",
    "        \"memory\": memory,\n",
    "        \"freeze_encoders\": freeze_encoders,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"patience\": patience,\n",
    "        \"warmup_epochs\": warmup_epochs,\n",
    "        \"lr_drop_after\": lr_drop_after,\n",
    "        \"device\": device,\n",
    "        \"random_state\": random_state,\n",
    "    }\n",
    "    uuid = str(abs(hash(tuple(df_dict.values()))))[:16]\n",
    "    log_file_name = f\"results_{uuid}.csv\"\n",
    "    if os.path.exists(os.path.join(results_dir, log_file_name)):\n",
    "        logger.warning(\n",
    "            f\"Results file {log_file_name} already exists. Skipping this run.\"\n",
    "        )\n",
    "        return None, None\n",
    "\n",
    "    # Generate the graph\n",
    "    graph = generate_graph(n_nodes=n_nodes, density=density, random_state=random_state)\n",
    "\n",
    "    # Generate the classification dataset\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = get_classification_data(\n",
    "        dataset_name\n",
    "    )\n",
    "\n",
    "    # Create the DataLoader instances\n",
    "    batch_size = 256\n",
    "    train_loader = get_dataloader(X_train, y_train, batch_size=batch_size)\n",
    "    val_loader = get_dataloader(X_val, y_val, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = get_dataloader(X_test, y_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    encoder_weights = os.path.join(\n",
    "        \"enc_dec_weights\", f\"{dataset_name}_encoder_weights.pth\"\n",
    "    )\n",
    "    if not os.path.exists(encoder_weights):\n",
    "        raise FileNotFoundError(f\"Pretrained weights not found for {dataset_name}.\")\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    encoder_model = FeedForwardNeuralNetwork(\n",
    "        in_features=X_train.shape[1],\n",
    "        out_features=n_classes,\n",
    "        hidden_dims=[],\n",
    "        dropout_rate=0.0,\n",
    "        use_batchnorm=False,\n",
    "    )\n",
    "    encoder_model.load_state_dict(torch.load(encoder_weights))\n",
    "    encoder_model.to(device)\n",
    "\n",
    "    # Create the model\n",
    "    model = MolComNetwork(\n",
    "        graph,\n",
    "        n_features=X_train.shape[1],\n",
    "        n_classes=n_classes,\n",
    "        n_iters=n_iters,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_batchnorm=use_batchnorm,\n",
    "        hidden_dims=hidden_dims,\n",
    "        residual_coefficient=residual_coefficient,\n",
    "        output_coefficient=output_coefficient,\n",
    "        encoder=encoder_model,\n",
    "        use_diffusion_noise=use_diffusion_noise,\n",
    "        memory=memory,\n",
    "        freeze_encoders=freeze_encoders,\n",
    "    )\n",
    "\n",
    "    if patience is None or patience < 0:\n",
    "        patience = n_epochs\n",
    "\n",
    "    # Train the model\n",
    "    train_metrics, test_metrics = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        n_epochs=n_epochs,\n",
    "        patience=patience,\n",
    "        warmup_epochs=warmup_epochs,\n",
    "        lr_drop_after=lr_drop_after,\n",
    "        device=device,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # Log the results\n",
    "    best_epoch = np.argmin(train_metrics[\"val\"][\"loss\"])\n",
    "    df_dict[\"best_epoch\"] = best_epoch\n",
    "    for key in train_metrics[\"train\"]:\n",
    "        df_dict[f\"train_{key}\"] = train_metrics[\"train\"][key][best_epoch]\n",
    "    for key in train_metrics[\"val\"]:\n",
    "        df_dict[f\"val_{key}\"] = train_metrics[\"val\"][key][best_epoch]\n",
    "    for key in test_metrics:\n",
    "        df_dict[f\"test_{key}\"] = test_metrics[key]\n",
    "\n",
    "    pd.DataFrame([df_dict]).to_csv(\n",
    "        os.path.join(results_dir, log_file_name), index=False\n",
    "    )\n",
    "\n",
    "    return train_metrics, test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 20:30:14,993\tINFO worker.py:1852 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ray for parallel processing\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def single_run_remote(**kwargs):\n",
    "    return single_run(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments on Ideal and Diffusion Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of experiments: 72. Time: 2025-07-01 20:30:19.925107\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m experiment_params:\n\u001b[32m     24\u001b[39m     results.append(\n\u001b[32m     25\u001b[39m         single_run_remote.remote(\n\u001b[32m     26\u001b[39m             **params,\n\u001b[32m     27\u001b[39m             device=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m         )\n\u001b[32m     29\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m results = \u001b[43mray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\commence\\Lib\\site-packages\\ray\\_private\\auto_init_hook.py:21\u001b[39m, in \u001b[36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mauto_init_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     20\u001b[39m     auto_init_ray()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\commence\\Lib\\site-packages\\ray\\_private\\client_mode_hook.py:103\u001b[39m, in \u001b[36mclient_mode_hook.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func.\u001b[34m__name__\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33minit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[32m    102\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func.\u001b[34m__name__\u001b[39m)(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\commence\\Lib\\site-packages\\ray\\_private\\worker.py:2782\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(object_refs, timeout)\u001b[39m\n\u001b[32m   2776\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2777\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid type of object refs, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(object_refs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, is given. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2778\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mobject_refs\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must either be an ObjectRef or a list of ObjectRefs. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2779\u001b[39m     )\n\u001b[32m   2781\u001b[39m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2782\u001b[39m values, debugger_breakpoint = \u001b[43mworker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2783\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[32m   2784\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\commence\\Lib\\site-packages\\ray\\_private\\worker.py:903\u001b[39m, in \u001b[36mWorker.get_objects\u001b[39m\u001b[34m(self, object_refs, timeout, return_exceptions, skip_deserialization)\u001b[39m\n\u001b[32m    893\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    894\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempting to call `get` on the value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_ref\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    895\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mwhich is not an ray.ObjectRef.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    896\u001b[39m         )\n\u001b[32m    898\u001b[39m timeout_ms = (\n\u001b[32m    899\u001b[39m     \u001b[38;5;28mint\u001b[39m(timeout * \u001b[32m1000\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m timeout != -\u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m -\u001b[32m1\u001b[39m\n\u001b[32m    900\u001b[39m )\n\u001b[32m    901\u001b[39m data_metadata_pairs: List[\n\u001b[32m    902\u001b[39m     Tuple[ray._raylet.Buffer, \u001b[38;5;28mbytes\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m ] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcore_worker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    908\u001b[39m debugger_breakpoint = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    909\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m data, metadata \u001b[38;5;129;01min\u001b[39;00m data_metadata_pairs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpython\\\\ray\\\\_raylet.pyx:3211\u001b[39m, in \u001b[36mray._raylet.CoreWorker.get_objects\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpython\\\\ray\\\\includes/common.pxi:83\u001b[39m, in \u001b[36mray._raylet.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Define the parameters for the set of experiments\n",
    "param_lists = {\n",
    "    \"random_state\": [42 + i for i in range(3)],\n",
    "    #\"n_nodes\": [4, 8, 16, 32, 48, 64],\n",
    "    \"n_nodes\": [4, 8],\n",
    "    \"dataset_name\": [\"breast_cancer\", \"iris\", \"wine\"],\n",
    "    \"n_iters\": [1, 2],\n",
    "    \"use_diffusion_noise\": [True, False],\n",
    "    \"results_dir\": [\"results_cls\"],\n",
    "}\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "experiment_params = list(\n",
    "    dict(zip(param_lists.keys(), values)) for values in product(*param_lists.values())\n",
    ")\n",
    "\n",
    "# Shuffle the parameters\n",
    "shuffle(experiment_params)\n",
    "print(f\"Number of experiments: {len(experiment_params)}. Time: {datetime.now()}\")\n",
    "\n",
    "# Run the experiments in parallel\n",
    "results = []\n",
    "for params in experiment_params:\n",
    "    results.append(\n",
    "        single_run_remote.remote(\n",
    "            **params,\n",
    "            device=\"cpu\",\n",
    "        )\n",
    "    )\n",
    "results = ray.get(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments on Pure Relay Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the set of experiments\n",
    "param_lists = {\n",
    "    \"random_state\": [42 + i for i in range(3)],\n",
    "    #\"n_nodes\": [4, 8, 16, 32, 48, 64],\n",
    "    \"n_nodes\": [4, 8],\n",
    "    \"dataset_name\": [\"breast_cancer\", \"iris\", \"wine\"],\n",
    "    \"n_iters\": [1, 2],\n",
    "    \"residual_coefficient\": [1.0],\n",
    "    \"output_coefficient\": [0.0],\n",
    "    \"results_dir\": [\"results_cls_relay\"],\n",
    "}\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "experiment_params = list(\n",
    "    dict(zip(param_lists.keys(), values)) for values in product(*param_lists.values())\n",
    ")\n",
    "\n",
    "# Shuffle the parameters\n",
    "shuffle(experiment_params)\n",
    "print(f\"Number of experiments: {len(experiment_params)}. Time: {datetime.now()}\")\n",
    "\n",
    "# Run the experiments in parallel\n",
    "results = []\n",
    "for params in experiment_params:\n",
    "    results.append(\n",
    "        single_run_remote.remote(\n",
    "            **params,\n",
    "            device=\"cpu\",\n",
    "        )\n",
    "    )\n",
    "results = ray.get(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect and Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(path, csv_file=None) -> pd.DataFrame:\n",
    "    if type(path) != list:\n",
    "        path = [path]\n",
    "    dfs = []\n",
    "    for p in path:\n",
    "        for file in tqdm(glob.glob(os.path.join(p, \"*.csv\"))):\n",
    "            dfs.append(pd.read_csv(file))\n",
    "    if len(dfs) == 0:\n",
    "        raise ValueError(f\"No files found in {path}\")\n",
    "    results_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Drop rows with freeze_encoders is False\n",
    "    if \"freeze_encoders\" in results_df.columns:\n",
    "        results_df = results_df[results_df[\"freeze_encoders\"] == True]\n",
    "\n",
    "    # Delete columns with constant values\n",
    "    results_df = results_df.loc[:, results_df.apply(pd.Series.nunique) != 1]\n",
    "\n",
    "    # Delete columns with loss or acc different from test\n",
    "    results_df = results_df.loc[:, ~results_df.columns.str.contains(\"train|val\")]\n",
    "\n",
    "    if csv_file is not None:\n",
    "        # Save the dataframe to a CSV file\n",
    "        results_df.to_csv(csv_file, index=False)\n",
    "        print(f\"Results saved to {csv_file}\")\n",
    "    return results_df\n",
    "\n",
    "\n",
    "@cache\n",
    "def load_baseline() -> pd.DataFrame:\n",
    "    df = load_results(\"results_cls_relay\")\n",
    "    # Filter only numerical columns\n",
    "    _df = df.select_dtypes(include=[np.number])\n",
    "    _df[\"dataset_name\"] = df[\"dataset_name\"]\n",
    "    # Average every numerical column grouping by dataset_name\n",
    "    df = _df.groupby([\"dataset_name\"]).mean().reset_index()\n",
    "    df.drop(\n",
    "        columns=[\"n_nodes\", \"n_iters\", \"random_state\", \"test_loss\", \"test_acc_avg\"],\n",
    "        inplace=True,\n",
    "    )\n",
    "    df.rename(\n",
    "        columns={\"dataset_name\": \"dataset\", \"test_acc_last\": \"acc_nl\"}, inplace=True\n",
    "    )\n",
    "\n",
    "    df_clf_baseline = pd.read_csv(\n",
    "        os.path.join(\"enc_dec_weights\", \"enc_dec_results.csv\")\n",
    "    )\n",
    "    # Join dataframe on dataset\n",
    "    df_clf_baseline = df_clf_baseline.merge(df, on=\"dataset\", how=\"right\")\n",
    "    return df_clf_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline = load_baseline()\n",
    "df_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_results([\"results_cls\"], csv_file=os.path.join(\"results_cls.csv\"))\n",
    "df_res = df[df[\"use_diffusion_noise\"] == True]\n",
    "df_res_nn = df[df[\"use_diffusion_noise\"] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_exp(ax, dataset, df, df_baseline, axes):\n",
    "    subset = df[df[\"dataset_name\"] == dataset]\n",
    "    n_iter = subset[\"n_iters\"].unique()[0]\n",
    "    sns.lineplot(\n",
    "        data=subset,\n",
    "        x=\"n_nodes\",\n",
    "        y=\"test_acc_last\",\n",
    "        hue=\"n_iters\",\n",
    "        ax=ax,\n",
    "        palette=\"tab10\",\n",
    "        marker=\"o\",\n",
    "    )\n",
    "\n",
    "    # Look up random‐guess and encoder–decoder acc from data_df\n",
    "    row = df_baseline.loc[df_baseline[\"dataset\"] == dataset].iloc[0]\n",
    "    rand_guess = 1.0 / row[\"n_classes\"]\n",
    "    enc_dec_acc = row[\"acc\"]\n",
    "    no_learning_acc = row[\"acc_nl\"]\n",
    "\n",
    "    # Draw dashed lines; only label them on the first subplot\n",
    "    if ax is axes[0]:\n",
    "        ax.axhline(enc_dec_acc, linestyle=\"--\", color=\"black\", label=\"Opt\")\n",
    "        ax.axhline(no_learning_acc, linestyle=\"--\", color=\"gray\", label=\"No Learning\")\n",
    "    else:\n",
    "        ax.axhline(enc_dec_acc, linestyle=\"--\", color=\"black\")\n",
    "        ax.axhline(no_learning_acc, linestyle=\"--\", color=\"gray\")\n",
    "\n",
    "    # Formatting\n",
    "    ax.get_legend().remove()\n",
    "    ax.set_ylim(rand_guess - 0.01, 1.01)\n",
    "    ax.set_xlim(df[\"n_nodes\"].min(), df[\"n_nodes\"].max())\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "\n",
    "def plot_results(df_res: pd.DataFrame, df_res_nn: pd.DataFrame, output_fig: str = None):\n",
    "    num_datasets = len(df_res[\"dataset_name\"].unique())\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=num_datasets,\n",
    "        ncols=2,\n",
    "        figsize=(6.3, 2 * num_datasets),\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "    )\n",
    "    fig.tight_layout()\n",
    "\n",
    "    for i, df in enumerate([df_res_nn, df_res]):\n",
    "        for j, dataset in enumerate(sorted(df[\"dataset_name\"].unique())):\n",
    "            plot_single_exp(axes[j, i], dataset, df, df_baseline, axes)\n",
    "\n",
    "    for j, dataset in enumerate(sorted(df_res[\"dataset_name\"].unique())):\n",
    "        axes[j, 0].set_ylabel(\n",
    "            f\"Test Accuracy\\non {' '.join(dataset.split('_')).title()}\"\n",
    "        )\n",
    "\n",
    "    for i in range(2):\n",
    "        axes[-1, i].set_xlabel(\"Number of Nodes\")\n",
    "\n",
    "    axes[0, 0].set_title(\"Ideal Channel\")\n",
    "    axes[0, 1].set_title(\"Diffusion Channel\")\n",
    "\n",
    "    # Build a global legend (includes both the line‐plot handles and our dashed‐line handles)\n",
    "    handles, labels = axes[-1, -1].get_legend_handles_labels()\n",
    "\n",
    "    handles, labels = axes[-1, -1].get_legend_handles_labels()\n",
    "\n",
    "    # Create explicit dashed‐line handles\n",
    "    opt_handle = Line2D([0], [0], color=\"black\", linestyle=\"--\", label=\"Optimal\")\n",
    "    no_learn_handle = Line2D([0], [0], color=\"gray\", linestyle=\"--\", label=\"Relay\")\n",
    "\n",
    "    # Append them\n",
    "    handles.extend([opt_handle, no_learn_handle])\n",
    "    labels.extend([\"Optimal\", \"Relay\"])\n",
    "\n",
    "    fig.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        loc=\"upper right\",\n",
    "        bbox_to_anchor=(0.94, -0.03),\n",
    "        title=\"Iterations\",\n",
    "        ncol=6,\n",
    "    )\n",
    "    if output_fig is not None:\n",
    "        plt.savefig(output_fig, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(df_res, df_res_nn, output_fig=os.path.join(\"results_cls.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MolComEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A custom Gym environment for Molecular Communication with node failures.\n",
    "    The agent learns to ensure message delivery despite node outages.\n",
    "    \"\"\"\n",
    "    metadata = {'render_modes': ['human'], 'render_fps': 30}\n",
    "\n",
    "    def __init__(self, dataset_name='iris', n_nodes=10, n_iters=5):\n",
    "        \"\"\"\n",
    "        Initializes the MolComEnv environment.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # The name of the dataset to use for classification tasks, defines features and classes\n",
    "        self.dataset_name = dataset_name \n",
    "        # Total number of nodes in MolCom graph\n",
    "        self.n_nodes = n_nodes\n",
    "        # Max number of iterations per episode\n",
    "        self.n_iters = n_iters \n",
    "        # current iteration in episode\n",
    "        self.current_step = 0 \n",
    "        \n",
    "        # Define paths for loading pre-trained encoder/decoder weights\n",
    "        self.script_dir = os.getcwd()\n",
    "        self.enc_dec_weights_dir = os.path.join(self.script_dir, \"enc_dec_weights\")\n",
    "\n",
    "        # Load data (for message content and classification task)\n",
    "        self.X_train, self.X_val, self.X_test, self.y_train, self.y_val, self.y_test = \\\n",
    "            get_classification_data(dataset_name)\n",
    "\n",
    "        n_features = self.X_train.shape[1]\n",
    "        n_classes = len(np.unique(self.y_train))\n",
    "\n",
    "        # Generate graph topology or node positions for communication\n",
    "        graph = generate_graph(self.n_nodes, density=0.5, random_state=42)\n",
    "\n",
    "        # Load the pre-trained encoder\n",
    "        encoder = FeedForwardNeuralNetwork(\n",
    "            in_features=n_features,\n",
    "            out_features=n_classes,  \n",
    "            hidden_dims=[] \n",
    "        )\n",
    "        encoder_weights_path = os.path.join(self.enc_dec_weights_dir, f\"{dataset_name}_encoder_weights.pth\")\n",
    "\n",
    "        # Load weights that were saved\n",
    "        if os.path.exists(encoder_weights_path):\n",
    "            encoder.load_state_dict(torch.load(encoder_weights_path, map_location='cpu'))\n",
    "            encoder.eval()  \n",
    "            logger.info(f\"Loaded pre-trained encoder for {dataset_name}.\")\n",
    "        else:\n",
    "            logger.warning(f\"Pre-trained encoder not found at {encoder_weights_path}. \"\n",
    "                           \"MolComNetwork will attempt to run without it.\")\n",
    "            encoder = None  \n",
    "            \n",
    "        # Initialize Molecular Communication Network model for message passing, phi networks, and diffusion\n",
    "        self.molcom_network = MolComNetwork(\n",
    "            graph=graph,\n",
    "            n_features=n_features,\n",
    "            n_classes=n_classes,\n",
    "            encoder=encoder,  \n",
    "            n_iters=self.n_iters,\n",
    "            hidden_dims=[4],\n",
    "            use_diffusion_noise=True,\n",
    "            memory=True,\n",
    "            freeze_encoders=True  \n",
    "        )\n",
    "\n",
    "        # Define action and observation spaces\n",
    "        # Multibinary(n_nodes) refers to the agent outputing a binary action for each node (i.e. relay strength = 1 or 0)\n",
    "        self.action_space = spaces.MultiBinary(self.n_nodes)\n",
    "\n",
    "        self.observation_space = spaces.Dict({\n",
    "            # Space for continuous values referring to message concentration at each node\n",
    "            \"messages\": spaces.Box(low=-np.inf, high=np.inf, shape=(self.n_nodes, n_classes), dtype=np.float32),\n",
    "            # A binary vector showing which nodes are operational\n",
    "            \"active_nodes_mask\": spaces.MultiBinary(self.n_nodes)\n",
    "        })\n",
    "        # Tracking which nodes are currently operational\n",
    "        self.active_nodes_mask = np.ones(self.n_nodes, dtype=np.int8)\n",
    "        # Message concentration across all nodes at the beginning of an iteration\n",
    "        self.current_message_state = torch.zeros(1, self.n_nodes, n_classes, dtype=torch.float32)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"Helper to get current observation.\"\"\"\n",
    "        return {\n",
    "            \"messages\": self.current_message_state.squeeze(0).detach().numpy(),\n",
    "            \"active_nodes_mask\": self.active_nodes_mask\n",
    "        }\n",
    "\n",
    "    def _get_info(self):\n",
    "        \"\"\"Helper to get current info.\"\"\"\n",
    "        return {\"current_step\": self.current_step}\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Reset the environment to an initial state.\n",
    "        A new message is generated, and a random node might fail.\n",
    "        \"\"\"\n",
    "        # Call to parent class' reset method to handle seeding for RNG\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # All nodes active at start (all 1)\n",
    "        self.active_nodes_mask = np.ones(self.n_nodes, dtype=np.int8)\n",
    "\n",
    "        # Random node failure at 20% probability at beginning of episode\n",
    "        if self.np_random.random() < 0.2:\n",
    "            # Selecting random node to fail\n",
    "            failed_node_idx = self.np_random.integers(0, self.n_nodes)\n",
    "            # Mark current node as inactive\n",
    "            self.active_nodes_mask[failed_node_idx] = 0\n",
    "            #logger.info(f\"Node {failed_node_idx} has failed at reset.\")\n",
    "\n",
    "        # Random data sample is selected from training set\n",
    "        sample_idx = self.np_random.integers(0, len(self.X_train))\n",
    "        self.current_input = torch.tensor(self.X_train[sample_idx:sample_idx + 1], dtype=torch.float32)\n",
    "        self.current_target = torch.tensor(self.y_train[sample_idx:sample_idx + 1], dtype=torch.long)\n",
    "\n",
    "        # Initialize message_state at Sender node\n",
    "        self.current_message_state = torch.zeros(1, self.n_nodes, self.molcom_network.n_classes, dtype=torch.float32)\n",
    "\n",
    "        # Encoder transforms input data into the chemical message\n",
    "        if self.molcom_network.encoder is not None:\n",
    "            self.current_message_state[:, self.molcom_network.sender, :] = self.molcom_network.encoder(\n",
    "                self.current_input)\n",
    "\n",
    "        # Applying ReLU as chemical concentrations are typically non-negative\n",
    "        self.current_message_state = torch.relu(self.current_message_state)\n",
    "\n",
    "        # Step counter of episode resets to 0\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Return the observation (messages, active nodes, current step)\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute one step in the environment.\n",
    "        \"\"\"\n",
    "        # Increment step counter for episode\n",
    "        self.current_step += 1\n",
    "        # Initialize reward\n",
    "        reward = 0.0\n",
    "        # Boolean for episode termination (failure) or truncation(max steps or iteration per episode)\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        \n",
    "        # Introduce node failure mid episode (10% chance)\n",
    "        if self.current_step > self.n_iters // 2 and self.np_random.random() < 0.1:\n",
    "            # Get current active nodes\n",
    "            active_indices = np.where(self.active_nodes_mask == 1)[0]\n",
    "            # Active indices need to be greater than 1 to ensure more than just sender/receiver are active. \n",
    "            if len(active_indices) > 1:\n",
    "                # Select random node to fail\n",
    "                failed_node_idx = self.np_random.integers(0, len(active_indices))\n",
    "                failed_node_idx_in_graph = active_indices[failed_node_idx]\n",
    "                self.active_nodes_mask[failed_node_idx_in_graph] = 0\n",
    "                #logger.info(f\"Node {failed_node_idx_in_graph} has failed at step {self.current_step}.\")\n",
    "\n",
    "        # Convert agent action to represent relay strength for each node (can be 1 or 0)\n",
    "        relay_strength = torch.tensor(action, dtype=torch.float32).unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "        # Copy state of current message and move it to device used by MolComNetwork distances\n",
    "        msg_for_step = self.current_message_state.clone().to(self.molcom_network.distances.device)\n",
    "\n",
    "        # Apply active node mask, if a node is inactive it there is no message contribution\n",
    "        msg_for_step = msg_for_step * torch.tensor(self.active_nodes_mask, device=msg_for_step.device,\n",
    "                                                   dtype=torch.float32).unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "        # Apply relay strength to message\n",
    "        msg_for_step = msg_for_step * relay_strength\n",
    "\n",
    "\n",
    "        # Pass message through 1 iteration of MolComNetwork's message_passing function\n",
    "        new_msg = message_passing(\n",
    "            msg_for_step,\n",
    "            self.molcom_network.distances,\n",
    "            self.molcom_network.phi,\n",
    "            residual_coefficient=self.molcom_network.residual_coefficient,\n",
    "            output_coefficient=self.molcom_network.output_coefficient,\n",
    "            memory=self.molcom_network.memory,\n",
    "            use_diffusion_noise=self.molcom_network.use_diffusion_noise,\n",
    "        )\n",
    "        new_msg = torch.relu(new_msg)\n",
    "\n",
    "        # Update the environment's message state\n",
    "        self.current_message_state = new_msg.clone()\n",
    "\n",
    "        # Calculate reward\n",
    "        receiver_output = self.current_message_state[:, self.molcom_network.receiver, :]\n",
    "        logits = receiver_output\n",
    "\n",
    "        \n",
    "        # # Track metrics\n",
    "        # loss = None\n",
    "        # accuracy = None\n",
    "\n",
    "        if self.molcom_network.n_classes > 1:\n",
    "            # Get most probable class\n",
    "            predicted_class = logits.argmax(dim=1) \n",
    "            # Check if class matches labelled target\n",
    "            is_correct = (predicted_class == self.current_target.to(logits.device)).float().item()\n",
    "\n",
    "            accuracy = is_correct\n",
    "            \n",
    "\n",
    "        # info.update({\n",
    "        #     \"loss\": loss,\n",
    "        #     \"accuracy\": accuracy,\n",
    "        #     \"step_reward\": reward,\n",
    "        #     \"active_nodes\": int(self.active_nodes_mask.sum())\n",
    "        # })\n",
    "        \n",
    "            # Reward for correct classification\n",
    "            reward = is_correct * 10.0\n",
    "        \n",
    "            # Penalty for inactive nodes\n",
    "            reward -= (1 - self.active_nodes_mask).sum() * 0.1\n",
    "        \n",
    "            # if receiver node fails episode ends immediately and they is a reward penalty\n",
    "            if self.active_nodes_mask[self.molcom_network.receiver] == 0:\n",
    "                reward -= 5.0\n",
    "                terminated = True\n",
    "        else:\n",
    "            # Reward is negative otherwise using MSE error\n",
    "            reward = -torch.nn.functional.mse_loss(logits, self.current_target.float().to(logits.device)).item()\n",
    "        # Check for early termination\n",
    "        if self.current_step >= self.n_iters:\n",
    "            terminated = True\n",
    "\n",
    "            final_logits = receiver_output\n",
    "            if self.molcom_network.n_classes > 1:\n",
    "                final_predicted_class = final_logits.argmax(dim=1)\n",
    "                final_is_correct = (final_predicted_class == self.current_target.to(logits.device)).float().item()\n",
    "                # Significant reward for correct classification at the end of episode, valuing correct message for robust end-to-end communication.\n",
    "                reward += final_is_correct * 50.0\n",
    "        \n",
    "        # Return the observation (messages, active nodes, current step)\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Create a callback class to record losses\n",
    "class LossRecordingCallback(BaseCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.policy_losses = []\n",
    "        self.value_losses  = []\n",
    "        self.entropy_losses = []\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        # SB3 logger stores the last rollout's scalars here:\n",
    "        logs = self.model.logger.name_to_value\n",
    "        # append each scalar to our lists\n",
    "        self.policy_losses.append(logs[\"train/policy_loss\"])\n",
    "        self.value_losses.append(logs[\"train/value_loss\"])\n",
    "        self.entropy_losses.append(logs[\"train/entropy_loss\"])\n",
    "\n",
    "def train_rl_agent():\n",
    "    \n",
    "    # Using make_vec_env can run multiple environments in parallel for faster training (4 by default)\n",
    "    env = make_vec_env(MolComEnv, n_envs=4, seed=0, env_kwargs={'dataset_name': 'iris', 'n_nodes': 6, 'n_iters': 8})\n",
    "    \n",
    "    \n",
    "    # Defining a model and using \"MultiInputPolicy\" for dictionary-based observation space\n",
    "    model = PPO(\"MultiInputPolicy\", env, verbose=1, learning_rate=1e-4, gamma=0.99, tensorboard_log=\"./ppo_molcom_log/\")\n",
    "\n",
    "    # instantiate our loss‐recorder\n",
    "    loss_cb = LossRecordingCallback()\n",
    "    \n",
    "    # Training the agent\n",
    "    logger.info(\"Starting RL agent training...\")\n",
    "    model.learn(total_timesteps=10000, callback=loss_cb, log_interval=1)  # PLEASE CHANGE THIS TO INCREASE TRAINING ITERATIONS, default value for demonstration only\n",
    "    logger.info(\"RL agent training finished.\")\n",
    "\n",
    "    # Saving the trained agent \n",
    "    model.save(\"molcom_rl_agent\")\n",
    "    logger.info(\"RL agent saved as molcom_rl_agent.zip\")\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(loss_cb.policy_losses, label=\"Policy Loss\")\n",
    "    plt.plot(loss_cb.value_losses,  label=\"Value Loss\")\n",
    "    plt.plot(loss_cb.entropy_losses, label=\"Entropy Loss\")\n",
    "    plt.xlabel(\"Rollout Number\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"PPO Training Losses Over Rollouts\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluating the trained agent \n",
    "    logger.info(\"Evaluating the trained RL agent...\")\n",
    "    eval_env = MolComEnv(dataset_name='iris', n_nodes=6, n_iters=8)  \n",
    "    obs, _ = eval_env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Number of episodes to evaluate\n",
    "    episodes = 5  \n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            # Predict action based on current observation, choose most probable action\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            # Take 1 step in environment with a choosen action\n",
    "            obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "            # Episode ends if a receiver node failed or max steps are reached\n",
    "            done = terminated or truncated\n",
    "            # Accumulate reward for the episode\n",
    "            episode_reward += reward\n",
    "        # Add episode reward to total\n",
    "        total_reward += episode_reward\n",
    "        logger.info(f\"Evaluation Episode {episode + 1}: Total Reward = {episode_reward:.2f}\")\n",
    "        # Reset for next episode\n",
    "        obs, _ = eval_env.reset()  \n",
    "    logger.info(f\"Average reward over {episodes} evaluation episodes: {total_reward / episodes:.2f}\")\n",
    "    eval_env.close()  # Close evaluation environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 19:55:16,636 - INFO - Starting data preprocessing and encoder-decoder training.\n",
      "2025-07-01 19:55:16,639 - INFO - Loaded existing results from C:\\Users\\New\\Desktop\\Commence\\commence-tmbc\\enc_dec_weights\\enc_dec_results.csv\n",
      "2025-07-01 19:55:16,640 - INFO - Encoder-decoder weights already exist for iris. Skipping training.\n",
      "2025-07-01 19:55:16,641 - INFO - Encoder-decoder weights already exist for wine. Skipping training.\n",
      "2025-07-01 19:55:16,642 - INFO - Encoder-decoder weights already exist for breast_cancer. Skipping training.\n",
      "2025-07-01 19:55:16,643 - INFO - All specified datasets processed for encoder-decoder training.\n",
      "2025-07-01 19:55:16,644 - INFO - Attempting to run RL agent training...\n",
      "2025-07-01 19:55:16,649 - INFO - Loaded pre-trained encoder for iris.\n",
      "2025-07-01 19:55:16,656 - INFO - Loaded pre-trained encoder for iris.\n",
      "2025-07-01 19:55:16,664 - INFO - Loaded pre-trained encoder for iris.\n",
      "2025-07-01 19:55:16,670 - INFO - Loaded pre-trained encoder for iris.\n",
      "2025-07-01 19:55:16,678 - INFO - Starting RL agent training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Encoder-Decoder Training Results ---\n",
      "         dataset  n_features  n_classes       acc\n",
      "0           iris           4          3  1.000000\n",
      "1           wine          13          3  1.000000\n",
      "2  breast_cancer          30          2  0.982456\n",
      "--------------------------------------\n",
      "Using cpu device\n",
      "Logging to ./ppo_molcom_log/PPO_15\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.42     |\n",
      "|    ep_rew_mean     | 30.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 445      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 7.65         |\n",
      "|    ep_rew_mean          | 37.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 410          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044163717 |\n",
      "|    clip_fraction        | 0.0139       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.16        |\n",
      "|    explained_variance   | -0.00119     |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 770          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00176     |\n",
      "|    value_loss           | 1.48e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 19:56:01,793 - INFO - RL agent training finished.\n",
      "2025-07-01 19:56:01,805 - INFO - RL agent saved as molcom_rl_agent.zip\n",
      "2025-07-01 19:56:01,805 - INFO - Evaluating the trained RL agent...\n",
      "2025-07-01 19:56:01,810 - INFO - Loaded pre-trained encoder for iris.\n",
      "2025-07-01 19:56:01,836 - INFO - Evaluation Episode 1: Total Reward = 130.00\n",
      "2025-07-01 19:56:01,858 - INFO - Evaluation Episode 2: Total Reward = 130.00\n",
      "2025-07-01 19:56:01,880 - INFO - Evaluation Episode 3: Total Reward = -0.80\n",
      "2025-07-01 19:56:01,901 - INFO - Evaluation Episode 4: Total Reward = -0.30\n",
      "2025-07-01 19:56:01,923 - INFO - Evaluation Episode 5: Total Reward = 129.70\n",
      "2025-07-01 19:56:01,925 - INFO - Average reward over 5 evaluation episodes: 77.72\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Starting data preprocessing and encoder-decoder training.\")\n",
    "    # Get current directory\n",
    "    script_dir = os.getcwd()\n",
    "    # Define directory for weights\n",
    "    enc_dec_weights_dir = os.path.join(script_dir, \"enc_dec_weights\")\n",
    "    # Create a directory if it doesn't exist\n",
    "    os.makedirs(enc_dec_weights_dir, exist_ok=True)\n",
    "\n",
    "    # Path for results, CSV\n",
    "    res_csv_path = os.path.join(enc_dec_weights_dir, \"enc_dec_results.csv\")\n",
    "    if os.path.exists(res_csv_path):\n",
    "        df = pd.read_csv(res_csv_path)\n",
    "        logger.info(f\"Loaded existing results from {res_csv_path}\")\n",
    "    else:\n",
    "        # Create and save dataframe\n",
    "        df = pd.DataFrame(columns=[\"dataset\", \"n_features\", \"n_classes\", \"acc\"])\n",
    "        df.to_csv(res_csv_path, index=False)\n",
    "        logger.info(f\"Created new results file at {res_csv_path}\")\n",
    "\n",
    "    # Loop through each dataset to train or load encoder-decoder models\n",
    "    for dataset_name in DATASETS:\n",
    "        if dataset_name in [\"identity\", \"shift\", \"negate\"]:\n",
    "            logger.info(f\"Skipping dataset: {dataset_name}\")\n",
    "            continue\n",
    "\n",
    "        enc_path = os.path.join(enc_dec_weights_dir, f\"{dataset_name}_encoder_weights.pth\")\n",
    "        dec_path = os.path.join(enc_dec_weights_dir, f\"{dataset_name}_decoder_weights.pth\")\n",
    "\n",
    "        # Check if encoder/decoder weights exist already\n",
    "        if not os.path.exists(enc_path) or not os.path.exists(dec_path):\n",
    "            logger.info(f\"Training encoder-decoder for dataset: {dataset_name}\")\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test = get_classification_data(dataset_name)\n",
    "            n_features = X_train.shape[1]\n",
    "            n_classes = len(np.unique(y_train))\n",
    "            _, _, acc = train_encoder_decoder(\n",
    "                X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                n_features, n_classes, hidden_dims=[], use_batchnorm=False,\n",
    "                lr=1e-3, n_epochs=10000, patience=100, batch_size=1024,\n",
    "                device = \"cuda\" , weights_dir=enc_dec_weights_dir, verbose=True,\n",
    "            )\n",
    "            logger.info(f\"Finished training encoder-decoder on {dataset_name}. Test Accuracy: {acc:.4f}\")\n",
    "            new_row = pd.DataFrame(\n",
    "                {\"dataset\": [dataset_name], \"n_features\": [n_features], \"n_classes\": [n_classes], \"acc\": [acc]})\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                df = pd.concat([df, new_row], ignore_index=True)\n",
    "            df.to_csv(res_csv_path, index=False)\n",
    "            logger.info(f\"Updated results file: {res_csv_path}\")\n",
    "        else:\n",
    "            logger.info(f\"Encoder-decoder weights already exist for {dataset_name}. Skipping training.\")\n",
    "\n",
    "    logger.info(\"All specified datasets processed for encoder-decoder training.\")\n",
    "    print(\"\\n--- Encoder-Decoder Training Results ---\")\n",
    "    print(df)\n",
    "    print(\"--------------------------------------\")\n",
    "\n",
    "    logger.info(\"Attempting to run RL agent training...\")\n",
    "    try:\n",
    "        train_rl_agent()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during RL agent training: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
