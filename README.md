# ü¶† Emergent Molecular Communication

A preliminary study of emergent molecular communication protocols learned by graph-based agents in a diffusion channel environment.

## üìã Table of Contents

- [ü¶† Emergent Molecular Communication](#-emergent-molecular-communication)
  - [üìã Table of Contents](#-table-of-contents)
  - [‚úèÔ∏è Abstract](#Ô∏è-abstract)
  - [‚úâÔ∏è Authors](#Ô∏è-authors)
  - [üöÄ Usage](#-usage)
  - [üß† RL Portion](#-rl-portion)
    - [üß™ MolComEnv](#-MolComEnv)
    - [ü§ñ RL Agent Training](#-rl-agent-training)
---

## ‚úèÔ∏è Abstract

Biological organisms have developed sophisticated communication mechanisms to enhance their adaptability and survival, ranging from molecular signaling to complex linguistic structures. Understanding the evolution of these mechanisms can provide insights into both biological processes and artificial communication system design. As a first attempt in this direction, a preliminary system based on Graph Neural Networks (GNNs) and molecular diffusion models is presented in this paper to observe the emergence of Molecular Communication (MC) protocols among artificial agents. In particular, these agents interact according to a Lewis-game framework by exchanging molecules that diffuse in a fluidic environment. In this framework, a sender encodes and transmits symbols to a destination with the help of intermediate relay nodes, subject to the stochastic behavior of diffusion-based MC, which affects message integrity. Each of these nodes can tune its internal parameters to influence communication, mimicking the evolution of biological organisms such as bacteria. Preliminary numerical results suggest that this system can learn to evolve an effective MC protocol. The findings highlight the potential of such a system for investigating emergent MC, possibly leading to a better understanding of molecular biological systems and the optimization of bio-based and bio-inspired networked systems.

## ‚úâÔ∏è Authors

- **Alberto Archetti**<sup>1</sup>  
- **Gabriele Giusti**<sup>1</sup>  
- **Karthik R. Gorla**<sup>1,3</sup>  
- **Stefano Caputo**<sup>2</sup>  
- **Maurizio Magarini**<sup>1</sup>  
- **Matteo Matteucci**<sup>1</sup>  
- **Lorenzo Mucchi**<sup>2</sup>  
- **Massimiliano Pierobon**<sup>3</sup>  

<sup>1</sup>Department of Electronics, Information, and Bioengineering, Politecnico di Milano, Milan, Italy  
<sup>2</sup>Department of Information Engineering, Universit√† degli Studi di Firenze, Florence, Italy  
<sup>3</sup>School of Computing, University of Nebraska‚ÄìLincoln, Lincoln, NE, USA


## ‚öôÔ∏è Installation
1. Clone the repo
```bash
git clone https://github.com/<OWNER>/<REPO>.git
cd <REPO>
```
2. Create Conda environment
```bash
conda env create -f environment.yml
conda activate commence
```
3. Install Python dependencies
```bash
poetry install
```

## üöÄ Usage
Run the Jupyter notebook to train, evaluate, and collect metrics:
```bash
jupyter lab commence_model.ipynb
```

## üß† RL Portion
Reinforcement Learning for Adaptive Communication
To enhance the adaptability of the molecular communication system in dynamic environments, a Reinforcement Learning (RL) framework has been integrated. The RL agent learns to make decisions that influence message propagation, aiming to optimize communication accuracy even when network conditions change (e.g., due to node failures). This setup allows the system to mimic the adaptive behavior observed in biological organisms.

## üß™ MolComEnv
A custom environment using the Gymnasium libray, it simulates a molecular communication channel with dynamically changing conditions.

Nodes can become inactive during an episode which includes eight iterations by default. 

One iteration includes: 
1. An action application is applied to the current message state at each node.
2. Each nodes current message (n_classes) diffuses across the network to all other nodes. The diffused message is aggregated at each receiving node.
3. The aggregated message at each node is processed by a phi neural network from the base code. 
4. Messages generated by the phi networks become the updated view of the signal.
    -residual coefficient: Determines how much of the old message contributes to the state of the next iteration.
    -output coefficient: Determents how much of the newly generated message contributes to the state of the next iteration
5. The message state of the receiver is tracked, it is used to calculated the reward for that iteration or step.
6. State update, where the internal state including the current message and and active nodes are updated for the next step.

Dynamic Node Failures:  Nodes can become inactive at the beginning of an episode or dynamically during a step.

Observation Space: The agent perceives the environment through a dictionary-based observation space, providing comprehensive information about the network's state.
#### 1. messages (spaces.Box): 
A continuous array representing the current concentration or state of chemical messages at each node. Its shape is (n_nodes, n_classes), where n_classes is the dimensionality of the chemical message.
    


#### 2. active_nodes_mask (spaces.MultiBinary): 
A binary vector of shape (n_nodes,) indicating the operational status of each node (1 for active, 0 for failed).


Action Space:
The agent's decisions are represented by a spaces.MultiBinary vector of shape (n_nodes,). Each element in this vector corresponds to a specific node in the communication graph:

1. A value of 1 for a node indicates that the agent "activates" or "strengthens" its relaying capabilities for the current message.

2. A value of 0 suggests the node's relaying is "deactivated" or "weakened".

These actions directly influence the relay_strength applied to messages at each node during the message passing process, allowing the agent to dynamically control signal propagation.

Reward Function
    
1. A base reward of +10.0 is given if the message, after propagation, leads to a correct classification of the original input data at the receiver node. This encourages successful information transfer. 
2. A small penalty of -0.1 is applied for each node that the agent effectively "turns off" (sets its relay_strength to 0). This discourages unnecessary deactivation of potentially useful relay points and promotes network utilization. 
3. A significant negative reward of -5.0 is imposed, and the episode terminates, if the receiver node itself fails. This strongly penalizes failures that directly impede the primary goal of communication. 
4. A substantial bonus of +50.0 is awarded at the end of an episode if the final classification at the receiver is correct. This emphasizes the importance of end-to-end communication success over the entire duration of the message propagation.


## ü§ñ RL Agent Training
The RL agent is trained using the Proximal Policy Optimization (PPO) algorithm, a widely adopted and robust policy-gradient method implemented via the Stable Baselines3 library.
    
1. Policy Architecture: Given the dictionary-based observation space, a MultiInputPolicy is employed, which is adept at handling structured input observations.
    
2. Training Process: The agent learns by interacting with multiple parallel instances of the MolComEnv (configured via make_vec_env) over a specified number of total_timesteps (e.g., 1,000,000). During this process, it refines its policy to select optimal node activation strategies to maximize cumulative reward.
    
3. Hyperparameters: Key hyperparameters such as learning_rate (e.g., 1e-4) and gamma (discount factor, e.g., 0.99) are configured to control the learning dynamics. These can be tuned for optimal performance based on experimentation.

4. Logging: TensorBoard integration (tensorboard_log="./ppo_molcom_log/") facilitates comprehensive monitoring of the training progress, including various metrics like rewards, losses, and value estimates.

5. Evaluation and Visualization: Following the training phase, the performance of the learned RL policy is rigorously evaluated over a set number of num_eval_episodes, 50 by default. 